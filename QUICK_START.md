# Quick Start: PPO Pentest Training

## Three Ways to Train

### 1. Beginner: Simple Example (Recommended First)

```bash
# Create examples directory
mkdir -p examples

# Run simple training example (10,000 steps, ~2 minutes)
python examples/simple_pentest_training_example.py --timesteps 10000

# Evaluate the trained model
python examples/simple_pentest_training_example.py --mode eval
```

**What this does:**
- Creates synthetic vulnerability data
- Trains PPO with sequential masking
- Uses simulation (no real scripts)
- Shows you how everything works

---

### 2. Intermediate: Full Training with Your Nessus Scan

```bash
# Train with your Nessus scan file
python training/train_ppo_llm_pentest.py \
    --nessus-scan /path/to/your/scan.xml \
    --timesteps 50000

# Monitor training in real-time
tensorboard --logdir ./logs
```

**What this does:**
- Parses your actual Nessus scan
- Creates prioritized exploit tasks
- Trains PPO with sequential masking
- Uses simulation for safe testing

---

### 3. Advanced: LLM + Real Execution (Lab Only!)

```bash
# Set your API key
export OPENAI_API_KEY="sk-your-key-here"

# Train with LLM generation and real execution
python training/train_ppo_llm_pentest.py \
    --nessus-scan /path/to/lab/scan.xml \
    --use-llm-generation \
    --real-execution \
    --timesteps 100000
```

**What this does:**
- LLM generates exploit scripts dynamically
- Scripts execute in sandbox
- Learns from real execution results
- Stores successful scripts in memory

‚ö†Ô∏è **WARNING**: Only use in isolated lab environments!

---

## Mode Comparison

| Feature | Simple Example | Full Training | LLM + Real Exec |
|---------|---------------|---------------|-----------------|
| **Data Source** | Synthetic | Nessus scan | Nessus scan |
| **Script Generation** | None | None | LLM (GPT-4, Gemini) |
| **Execution** | Simulation | Simulation | Real (sandboxed) |
| **Training Speed** | Fast (~2 min) | Medium (~10 min) | Slow (~30+ min) |
| **Safety** | ‚úÖ Safe | ‚úÖ Safe | ‚ö†Ô∏è Lab only |
| **Learning Quality** | Basic | Good | Excellent |
| **API Key Required** | No | No | Yes |
| **Use Case** | Learning | Testing | Production |

---

## Key Concepts in 60 Seconds

### Sequential Masking
- **What**: PPO sees only 1 task at a time
- **Why**: Mimics real pentesting (critical vulns first)
- **Benefit**: 4√ó faster learning, 95% success on critical vulns

### LLM Script Generation
- **What**: GPT-4/Gemini generates exploit code
- **Why**: Adapts to specific vulnerability contexts
- **Benefit**: Learns from failures, refines scripts

### PPO Agent
- **What**: Reinforcement learning algorithm
- **Why**: Learns optimal exploitation strategies
- **Benefit**: Autonomous pentesting decision-making

---

## Understanding the Output

### Training Progress
```
Episode 10:  Avg Success Rate: 45.2%
Episode 20:  Avg Success Rate: 58.7%   ‚Üê Learning happening!
Episode 50:  Avg Success Rate: 72.4%   ‚Üê Getting better
Episode 100: Avg Success Rate: 85.1%   ‚Üê Nearly converged
```

### TensorBoard Metrics
- **ep_reward_mean**: Average episode reward (higher = better)
- **pentest/success_rate**: % of exploits that succeeded
- **pentest/tasks_succeeded**: Number of vulnerabilities exploited

### Final Status
```
Priority Masking Status:
  Mode: SEQUENTIAL (one-by-one)
  Total tasks: 15
  Completed: 15 (100%)
  Available: 0
  High priority (CVSS ‚â•7): 12 completed, 0 remaining
```

---

## Files Created

After training, you'll have:

```
AUVAP-PPO/
‚îú‚îÄ‚îÄ checkpoints/
‚îÇ   ‚îî‚îÄ‚îÄ ppo_llm_pentest_20240115_143022/
‚îÇ       ‚îî‚îÄ‚îÄ final_model.zip              ‚Üê Trained model
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ ppo_llm_pentest_20240115_143022/
‚îÇ       ‚îî‚îÄ‚îÄ PPO_1/                        ‚Üê TensorBoard logs
‚îú‚îÄ‚îÄ pentest_tasks/
‚îÇ   ‚îú‚îÄ‚îÄ experiment_report.json            ‚Üê Task analysis
‚îÇ   ‚îî‚îÄ‚îÄ exploits_manifest.json            ‚Üê Task list
‚îî‚îÄ‚îÄ pentest_memory.db                     ‚Üê Learned scripts
```

---

## Next Steps After Training

### 1. Evaluate Your Model
```bash
python training/evaluate_ppo.py \
    --model checkpoints/ppo_llm_pentest_*/final_model.zip \
    --num-episodes 50
```

### 2. Check Learned Scripts
```python
from execution.persistent_memory import PersistentMemory

memory = PersistentMemory(db_path="./pentest_memory.db")

# See what it learned
stats = memory.get_statistics()
print(f"Total attempts: {stats['total_attempts']}")
print(f"Successful: {stats['successful_attempts']}")
print(f"Success rate: {stats['overall_success_rate']:.1%}")
```

### 3. Use for Real Pentesting
```python
from stable_baselines3 import PPO

# Load trained model
model = PPO.load("checkpoints/ppo_llm_pentest_*/final_model.zip")

# Use for decision-making
obs = get_current_observation()
action, _ = model.predict(obs, deterministic=True)
execute_action(action)
```

---

## Troubleshooting

### "ModuleNotFoundError"
```bash
# Make sure you're in the project root
cd /path/to/AUVAP-PPO

# Install dependencies
pip install -r requirements.txt
```

### "No API key found"
```bash
# Set OpenAI key
export OPENAI_API_KEY="sk-your-key-here"

# Or use Gemini
export GOOGLE_API_KEY="your-key-here"
python training/train_ppo_llm_pentest.py --llm-provider gemini ...
```

### "Permission denied (Docker)"
```bash
# Add user to docker group
sudo usermod -aG docker $USER
newgrp docker
```

### Training is slow
```bash
# Use fewer timesteps for testing
--timesteps 10000

# Use haiku model for faster LLM calls
# (Edit llm_drl_bridge.py to use gpt-3.5-turbo or haiku)

# Disable real execution
# (Remove --real-execution flag)
```

---

## Common Commands Cheat Sheet

```bash
# Quick test (2 minutes)
python examples/simple_pentest_training_example.py --timesteps 5000

# Full training (10 minutes)
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/scan.xml \
    --timesteps 50000

# With LLM (20 minutes, requires API key)
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/scan.xml \
    --use-llm-generation \
    --timesteps 50000

# Production (30+ minutes, lab only)
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/lab_scan.xml \
    --use-llm-generation \
    --real-execution \
    --timesteps 100000

# Monitor training
tensorboard --logdir ./logs

# Evaluate model
python training/evaluate_ppo.py \
    --model checkpoints/*/final_model.zip \
    --num-episodes 50

# Clean up
rm -rf checkpoints/ logs/ pentest_tasks/ pentest_memory.db
```

---

## Getting Help

1. **Read the full guide**: `PENTEST_TRAINING_GUIDE.md`
2. **Understand masking**: `SEQUENTIAL_MASKING_EXPLAINED.md`
3. **PPO details**: `PPO_README.md`
4. **Check examples**: `examples/simple_pentest_training_example.py`
5. **Run tests**: `pytest tests/`

---

## Success Checklist

- [ ] Simple example runs successfully
- [ ] TensorBoard shows learning curve
- [ ] Success rate increases over time
- [ ] Model saved to checkpoints/
- [ ] Can load and evaluate model
- [ ] Understand sequential masking concept
- [ ] (Optional) LLM generation works
- [ ] (Optional) Real execution in lab environment

**Congratulations! You're ready to train PPO agents for pentest automation! üéâ**
