# Technical Specification: PriorityMaskedEnv for PPO Pentest Training

## 1. Markov Decision Process (MDP) Formulation

The environment implements a **finite-horizon, episodic, partially observable Markov Decision Process (POMDP)** with action masking:

**Formal Definition:**
```
MDP = (S, A, P, R, Î³, Ïâ‚€, T, M)
```

Where:
- **S**: State space (continuous, high-dimensional)
- **A**: Action space (discrete, finite)
- **P**: Transition probability function `P(s'|s,a)`
- **R**: Reward function `R(s,a,s')`
- **Î³**: Discount factor (typically 0.99)
- **Ïâ‚€**: Initial state distribution
- **T**: Episode horizon (variable length)
- **M**: Action mask function `M(s) â†’ {0,1}^|A|`

---

## 2. State Space (S)

### 2.1 Mathematical Definition

The state space is a continuous vector space:

```
S âŠ† â„^d where d = 4N + 3
```

Where:
- **N**: Number of exploit tasks (cardinality of task set)
- **d**: Observation dimensionality

### 2.2 State Vector Composition

The state vector `s_t âˆˆ S` at timestep `t` is constructed as:

```
s_t = [Ï†â‚(Ï„â‚), Ï†â‚‚(Ï„â‚‚), ..., Ï†_N(Ï„_N), g(Î“_t)]
```

Where:
- **Ï„áµ¢**: Task i from task set T = {Ï„â‚, Ï„â‚‚, ..., Ï„_N}
- **Ï†áµ¢(Ï„áµ¢)**: Feature extraction function for task i
- **g(Î“_t)**: Global state features at time t
- **Î“_t**: Environment global state

### 2.3 Per-Task Feature Representation

For each task Ï„áµ¢ âˆˆ T, the feature vector Ï†áµ¢(Ï„áµ¢) âˆˆ â„â´ is:

```
Ï†áµ¢(Ï„áµ¢) = [
    Ï€(Ï„áµ¢) / 100.0,           # Normalized priority score
    CVSS(Ï„áµ¢) / 10.0,          # Normalized CVSS score
    ğŸ™(Ï„áµ¢ âˆˆ C_t),              # Completion indicator
    ğŸ™(Ï„áµ¢ âˆˆ A_t)               # Availability indicator (mask)
]
```

Where:
- **Ï€(Ï„áµ¢)**: Priority score function (range: [0, 100])
- **CVSS(Ï„áµ¢)**: Common Vulnerability Scoring System score (range: [0, 10])
- **C_t**: Set of completed tasks at time t
- **A_t**: Set of available (unmasked) tasks at time t
- **ğŸ™(Â·)**: Indicator function (1 if true, 0 if false)

### 2.4 Global State Features

The global feature vector g(Î“_t) âˆˆ â„Â³ is:

```
g(Î“_t) = [
    |C_t| / N,                    # Task completion ratio
    |H_accessible(t)| / N,        # Network access ratio
    t / T_max                     # Temporal progress
]
```

Where:
- **|C_t|**: Cardinality of completed task set
- **H_accessible(t)**: Set of accessible network hosts at time t
- **T_max**: Maximum episode length (horizon)

### 2.5 State Space Properties

- **Dimensionality**: d = 4N + 3 (scales linearly with task count)
- **Bounds**: s_t âˆˆ [0, 1]^d (all features normalized to unit interval)
- **Type**: Continuous, bounded, fully observable within episode
- **Example**: For N=10 tasks, d = 43 dimensions

---

## 3. Action Space (A)

### 3.1 Mathematical Definition

The action space is a **discrete, finite set**:

```
A = {0, 1, 2, ..., N-1}
```

Where each action `a âˆˆ A` corresponds to selecting task Ï„_{a+1} for exploitation.

### 3.2 Action Semantics

Action `a_t` at timestep t represents:

```
a_t: A â†’ T
a_t(i) = Ï„_{i+1}   (select and execute exploit for task i+1)
```

### 3.3 Action Space Properties

- **Type**: Discrete (gym.spaces.Discrete)
- **Cardinality**: |A| = N (fixed per environment instance)
- **Structure**: Flat (non-hierarchical)
- **Constraints**: Subject to action masking M(s_t)

---

## 4. Action Masking Function (M)

### 4.1 Formal Definition

The action mask function M: S â†’ {0,1}^|A| determines valid actions:

```
M(s_t) = [mâ‚€(s_t), mâ‚(s_t), ..., m_{N-1}(s_t)]

Where máµ¢(s_t) = {
    0,  if Ï„áµ¢ âˆˆ C_t âˆ¨ Ï„áµ¢ âˆ‰ A_t     (masked)
    1,  if Ï„áµ¢ âˆ‰ C_t âˆ§ Ï„áµ¢ âˆˆ A_t     (valid)
}
```

### 4.2 Sequential Masking Policy

In **sequential mode** (default), the availability set A_t is restricted:

```
A_t = {Ï„_k} where k = argmax_{i: Ï„áµ¢ âˆ‰ C_t} Ï€(Ï„áµ¢)
```

This enforces **single-task exposure**: only the highest-priority uncompleted task is unmasked.

### 4.3 Parallel Masking Policy

In **parallel mode**, the availability set is unrestricted:

```
A_t = T \ C_t
```

All uncompleted tasks are simultaneously available.

### 4.4 Masking Integration with PPO

The masked action space A'_t âŠ† A at time t is:

```
A'_t = {a âˆˆ A : M(s_t)[a] = 1}
```

Policy sampling is restricted to A'_t:

```
Ï€(a|s_t) = {
    softmax(logits[a])/Z,  if a âˆˆ A'_t
    0,                      if a âˆ‰ A'_t
}

Where Z = Î£_{a' âˆˆ A'_t} softmax(logits[a'])
```

---

## 5. Transition Dynamics (P)

### 5.1 Deterministic Components

The environment exhibits **deterministic transitions** for most state components:

```
P(s_{t+1} | s_t, a_t) = Î´(s_{t+1} - f(s_t, a_t))
```

Where f is the deterministic transition function:

```
f(s_t, a_t) updates:
- C_{t+1} = C_t âˆª {Ï„_{a_t}} if execution attempted
- H_accessible(t+1) = H_accessible(t) âˆª H_compromised(a_t) if success
- Step counter: t â†’ t+1
```

### 5.2 Stochastic Components

**Exploit execution outcome** introduces stochasticity:

#### Mode A: Probabilistic Simulation

```
P(success | Ï„áµ¢) = {
    0.90,  if CVSS(Ï„áµ¢) â‰¥ 9.0
    0.75,  if 7.0 â‰¤ CVSS(Ï„áµ¢) < 9.0
    0.60,  if 4.0 â‰¤ CVSS(Ï„áµ¢) < 7.0
    0.40,  if CVSS(Ï„áµ¢) < 4.0
}
```

Stochastic transition:

```
success_t ~ Bernoulli(P(success | Ï„_{a_t}))
```

#### Mode B: Real Script Execution

```
success_t = Execute(script(Ï„_{a_t}), target(Ï„_{a_t}), timeout)
```

Outcome depends on:
- Script correctness
- Target vulnerability state
- Network conditions
- Timeout constraints

### 5.3 Transition Function Properties

- **Markovian**: P(s_{t+1}|s_t, a_t) (no memory beyond current state)
- **Partially stochastic**: Deterministic state updates + stochastic execution
- **Episodic**: Terminates at fixed conditions

---

## 6. Reward Function (R)

### 6.1 Mathematical Definition

The reward function R: S Ã— A Ã— S â†’ â„ is defined as:

```
R(s_t, a_t, s_{t+1}) = R_validity(a_t, s_t) + R_execution(a_t, s_t, s_{t+1}) + R_step
```

### 6.2 Reward Components

#### 6.2.1 Validity Reward

```
R_validity(a_t, s_t) = {
    0,              if M(s_t)[a_t] = 1  (valid action)
    -2 Ã— r_fail,    if M(s_t)[a_t] = 0  (invalid action)
}

Where r_fail = 10.0 (base failure penalty)
```

#### 6.2.2 Execution Reward

```
R_execution(a_t, s_t, s_{t+1}) = {
    r_success + Î² Ã— CVSS(Ï„_{a_t})/10.0,  if success_t = 1
    -r_fail,                              if success_t = 0
}

Where:
- r_success = 100.0 (base success reward)
- r_fail = 10.0 (base failure penalty)
- Î² = 50.0 (risk bonus weight)
```

**Risk-adjusted reward** for successful exploitation:

```
R_success(Ï„áµ¢) = 100 + 50 Ã— (CVSS(Ï„áµ¢)/10) âˆˆ [100, 150]
```

#### 6.2.3 Step Penalty

```
R_step = -1.0  (constant per timestep)
```

Encourages episode efficiency.

### 6.3 Total Reward Examples

**Example 1: Successful high-risk exploit**
```
Task: CVE-2017-0144 (EternalBlue), CVSS = 10.0
R = 0 + (100 + 50Ã—1.0) + (-1) = 149
```

**Example 2: Failed medium-risk exploit**
```
Task: CVE-2019-0708 (BlueKeep), CVSS = 7.5
R = 0 + (-10) + (-1) = -11
```

**Example 3: Invalid action (masked task)**
```
Task: Already completed
R = -20 + 0 + (-1) = -21
```

### 6.4 Expected Episode Return

For episode with length T and k successes:

```
G = Î£_{t=0}^{T-1} Î³^t R_t

Expected: E[G] â‰ˆ k Ã— 125 - (T-k) Ã— 11 - T
```

Where:
- k successes at avg CVSS 7.5: ~125 reward each
- (T-k) failures: -11 reward each
- T step penalties: -1 each

---

## 7. Episode Structure

### 7.1 Initialization

Episode begins with:

```
s_0 ~ Ïâ‚€, where Ïâ‚€(s) = Î´(s - s_init)

s_init = [
    [Ï€(Ï„â‚)/100, CVSS(Ï„â‚)/10, 0, ğŸ™(Ï„â‚ âˆˆ A_0)],
    [Ï€(Ï„â‚‚)/100, CVSS(Ï„â‚‚)/10, 0, ğŸ™(Ï„â‚‚ âˆˆ A_0)],
    ...,
    [Ï€(Ï„_N)/100, CVSS(Ï„_N)/10, 0, ğŸ™(Ï„_N âˆˆ A_0)],
    [0, 1, 0]  # Global: 0% progress, full access, step 0
]
```

### 7.2 Termination Conditions

Episode terminates when:

```
done_t = (|C_t| â‰¥ N) âˆ¨ (t â‰¥ T_max)
```

Where:
- **|C_t| â‰¥ N**: All tasks attempted (normal termination)
- **t â‰¥ T_max**: Maximum steps reached (truncation)

### 7.3 Episode Trajectory

Complete trajectory Ï„:

```
Ï„ = (s_0, a_0, r_0, s_1, a_1, r_1, ..., s_T, a_T, r_T)
```

### 7.4 Episode Length Statistics

- **Minimum length**: T_min = N (one action per task, all valid)
- **Maximum length**: T_max = 2N (default) or configured
- **Expected length**: E[T] â‰ˆ N Ã— 1.2 (accounting for invalid actions during learning)

---

## 8. Environment Properties

### 8.1 Observability

- **Type**: Fully observable (within episode scope)
- **Justification**: Agent sees complete task state, network state, and progress
- **Caveat**: Attack chain dependencies create partial observability across episodes

### 8.2 Determinism

- **Classification**: Stochastic
- **Sources of Stochasticity**:
  - Exploit execution outcomes (probabilistic or real-world variability)
  - Initial state distribution Ïâ‚€ (if randomized)

### 8.3 Episodic vs Continuing

- **Type**: Episodic
- **Episode boundary**: Clear start (reset) and termination conditions
- **No discount across episodes**: Each episode is independent

### 8.4 Action Space Structure

- **Type**: Discrete, finite
- **Cardinality**: O(N) where N = number of tasks
- **Masking**: Dynamic, state-dependent
- **Constraint complexity**: O(1) per state (sequential mode)

### 8.5 Reward Structure

- **Type**: Sparse + Dense
- **Sparse component**: Large rewards on success/failure
- **Dense component**: Per-step penalty
- **Horizon sensitivity**: Unbounded (no explicit discount in rewards)

### 8.6 State Space Complexity

- **Dimensionality**: Linear in N: O(4N + 3)
- **Scalability**: Handles 10-100 tasks effectively
- **Bottleneck**: Policy network capacity, not observation size

---

## 9. Gymnasium API Implementation

### 9.1 Interface Specification

The environment implements the **Gymnasium** (formerly OpenAI Gym) API:

```python
class PriorityMaskedEnv(gym.Env):
    """Gymnasium-compliant pentest environment"""

    # Required attributes
    action_space: gym.spaces.Discrete
    observation_space: gym.spaces.Box
    metadata: Dict[str, Any] = {'render.modes': ['human']}
```

### 9.2 Core Methods

#### 9.2.1 Reset

```python
def reset(
    self,
    seed: Optional[int] = None,
    options: Optional[Dict] = None
) -> Tuple[np.ndarray, Dict]:
    """
    Reset environment to initial state.

    Returns:
        observation: s_0 âˆˆ S (initial state)
        info: Auxiliary diagnostic information
    """
```

**Postconditions:**
- C_0 = âˆ… (no completed tasks)
- t = 0 (timestep reset)
- A_0 determined by masking policy

#### 9.2.2 Step

```python
def step(
    self,
    action: int
) -> Tuple[np.ndarray, float, bool, bool, Dict]:
    """
    Execute action and transition environment.

    Args:
        action: a_t âˆˆ A (discrete action index)

    Returns:
        observation: s_{t+1} âˆˆ S (next state)
        reward: R(s_t, a_t, s_{t+1}) âˆˆ â„
        terminated: Whether episode naturally ended
        truncated: Whether episode was cut off
        info: Auxiliary diagnostic information
    """
```

**State transition:**
```
(s_t, a_t) â†’ (s_{t+1}, r_t)
```

#### 9.2.3 Action Masking

```python
def action_masks(self) -> np.ndarray:
    """
    Return valid action mask for current state.

    Returns:
        mask: M(s_t) âˆˆ {0,1}^N (binary mask)
    """
```

Used by **Stable-Baselines3 MaskablePPO**.

### 9.3 Vectorization Support

Compatible with `VecEnv` wrappers:

```python
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv

# Single-process vectorization
env = DummyVecEnv([lambda: PriorityMaskedEnv(...) for _ in range(4)])

# Multi-process vectorization
env = SubprocVecEnv([lambda: PriorityMaskedEnv(...) for _ in range(4)])
```

**Parallelization**: 4-16 environments typical for PPO training

---

## 10. PPO Integration Specifics

### 10.1 Policy Architecture

**Input Layer:**
```
Observation: s_t âˆˆ â„^{4N+3}
```

**Hidden Layers (MLP):**
```
h_1 = ReLU(W_1 Ã— s_t + b_1),  W_1 âˆˆ â„^{64 Ã— d}
h_2 = ReLU(W_2 Ã— h_1 + b_2),  W_2 âˆˆ â„^{64 Ã— 64}
```

**Policy Head (Actor):**
```
logits = W_Ï€ Ã— h_2 + b_Ï€,  W_Ï€ âˆˆ â„^{N Ã— 64}

Ï€(a|s) = softmax(logits)[a] Ã— M(s)[a] / Z
```

**Value Head (Critic):**
```
V(s) = W_V Ã— h_2 + b_V,  W_V âˆˆ â„^{1 Ã— 64}
```

### 10.2 Training Hyperparameters

```python
PPO(
    policy="MlpPolicy",
    env=env,
    learning_rate=3e-4,         # Î± (Adam optimizer)
    n_steps=2048,               # Rollout length per env
    batch_size=64,              # Minibatch size
    n_epochs=10,                # Epochs per update
    gamma=0.99,                 # Discount factor Î³
    gae_lambda=0.95,            # GAE Î»
    clip_range=0.2,             # PPO clip Îµ
    ent_coef=0.01,              # Entropy bonus Î²_H
    vf_coef=0.5,                # Value loss weight
    max_grad_norm=0.5           # Gradient clipping
)
```

### 10.3 Advantage Estimation

**Generalized Advantage Estimation (GAE):**

```
A_t^GAE(Î³,Î») = Î£_{l=0}^{âˆ} (Î³Î»)^l Î´_{t+l}

Where:
Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)  (TD residual)
```

### 10.4 Policy Update Objective

**Clipped Surrogate Objective:**

```
L_CLIP(Î¸) = E_t[min(
    r_t(Î¸) Ã‚_t,
    clip(r_t(Î¸), 1-Îµ, 1+Îµ) Ã‚_t
)]

Where:
r_t(Î¸) = Ï€_Î¸(a_t|s_t) / Ï€_Î¸_old(a_t|s_t)  (probability ratio)
Îµ = 0.2  (clip range)
```

**Value Function Loss:**

```
L_VF(Î¸) = E_t[(V_Î¸(s_t) - V_target(s_t))Â²]
```

**Entropy Bonus:**

```
L_ENT(Î¸) = -E_t[H(Ï€_Î¸(Â·|s_t))]
         = -E_t[Î£_a Ï€_Î¸(a|s_t) log Ï€_Î¸(a|s_t)]
```

**Total Loss:**

```
L(Î¸) = -L_CLIP(Î¸) + c_1 L_VF(Î¸) - c_2 L_ENT(Î¸)

Where:
c_1 = 0.5  (value loss coefficient)
c_2 = 0.01 (entropy coefficient)
```

---

## 11. Computational Complexity

### 11.1 Time Complexity

**Per Step:**
- Observation construction: O(N)
- Action masking: O(N)
- Reward calculation: O(1)
- State transition: O(1)

**Total per step**: O(N)

**Per Episode:**
- Expected steps: E[T] â‰ˆ N
- Episode complexity: O(NÂ²)

### 11.2 Space Complexity

**State representation**: O(N)
**Action mask storage**: O(N)
**Episode trajectory**: O(NT) where T â‰ˆ N

**Total**: O(NÂ²) per episode

### 11.3 Training Scalability

**Forward pass (policy):**
```
O(batch_size Ã— d Ã— hidden_dim + hidden_dim Ã— N)
â‰ˆ O(B Ã— N Ã— H + H Ã— N)
â‰ˆ O(BNH) where B=64, H=64
```

**Backward pass (PPO update):**
```
O(n_steps Ã— n_epochs Ã— batch_size)
â‰ˆ O(2048 Ã— 10 Ã— 64) = O(1.3M operations per update)
```

---

## 12. Environment Variants

### 12.1 Sequential Mode (Default)

```
sequential_mode = True
|A'_t| = 1  (single task exposed)
```

**Properties:**
- Faster convergence (4Ã— empirically)
- Deterministic task ordering
- Mimics real pentesting workflow

### 12.2 Parallel Mode

```
sequential_mode = False
|A'_t| = |T \ C_t|  (all uncompleted tasks)
```

**Properties:**
- More exploration
- Flexible task ordering
- Slower convergence

### 12.3 Execution Modes

**Simulation Mode:**
```
use_real_execution = False
P(success|Ï„) = f(CVSS(Ï„))  (probabilistic)
```

**Real Execution Mode:**
```
use_real_execution = True
success ~ Execute(script(Ï„), target(Ï„))  (actual scripts)
```

---

## 13. Formal Environment Verification

### 13.1 Gym Compliance Check

```python
from stable_baselines3.common.env_checker import check_env

env = PriorityMaskedEnv(...)
check_env(env)  # Validates Gym API compliance
```

### 13.2 Key Invariants

1. **Action mask consistency**: âˆ€t: M(s_t)[a] = 1 âŸ¹ a is executable
2. **State bounds**: âˆ€t: s_t âˆˆ [0,1]^d
3. **Episode termination**: |C_T| = N âˆ¨ T = T_max
4. **Reward boundedness**: R âˆˆ [-21, 150]
5. **Markov property**: P(s_{t+1}|s_0,...,s_t, a_t) = P(s_{t+1}|s_t, a_t)

---

## 14. Performance Metrics

### 14.1 Environment Metrics

- **Episode length**: E[T]
- **Task completion rate**: |C_T| / N
- **Success rate**: (# successes) / |C_T|
- **Invalid action rate**: (# invalid actions) / T

### 14.2 Learning Metrics

- **Average return**: E[G] = E[Î£_t Î³^t R_t]
- **Success rate on critical vulns**: P(success | CVSS â‰¥ 9.0)
- **Policy entropy**: H(Ï€(Â·|s))
- **Value function error**: E[(V(s) - V_target(s))Â²]

### 14.3 Convergence Criteria

Training converges when:

```
E[G] > threshold (e.g., 0.8 Ã— N Ã— 125)  AND
Ïƒ[G] < variance_threshold  AND
Invalid_rate < 0.05
```

---

## 15. Mathematical Properties

### 15.1 State Space Geometry

- **Manifold**: Hypercube [0,1]^d
- **Reachable states**: Dense subset of S (not all corners reachable)
- **State trajectory**: Piecewise linear path through S

### 15.2 Reward Distribution

**Empirical distribution** from simulation:

```
R ~ Mixture(
    pâ‚ Ã— Uniform(100, 150),    # Success outcomes
    pâ‚‚ Ã— Î´(-11),                # Failure outcomes
    pâ‚ƒ Ã— Î´(-21)                 # Invalid actions
)

Where: pâ‚ â‰ˆ 0.7, pâ‚‚ â‰ˆ 0.25, pâ‚ƒ â‰ˆ 0.05 (after learning)
```

### 15.3 Policy Convergence

**Optimal policy** Ï€* in sequential mode:

```
Ï€*(a|s) = {
    1, if a = argmax_{i: M(s)[i]=1} CVSS(Ï„áµ¢)
    0, otherwise
}
```

This is a **deterministic, greedy policy** w.r.t. CVSS score.

---

## 16. Implementation Details

### 16.1 Dependencies

```
gymnasium >= 0.28.0
numpy >= 1.21.0
stable-baselines3 >= 2.0.0
torch >= 1.12.0
```

### 16.2 Numerical Stability

- All observations normalized to [0, 1]
- Rewards scaled to [-21, 150] range
- No exponential terms (prevents overflow)
- Integer counters for completion tracking

### 16.3 Reproducibility

```python
env = PriorityMaskedEnv(...)
env.reset(seed=42)  # Sets environment RNG seed
```

PPO training seed:
```python
set_random_seed(42)  # Sets global seeds (NumPy, PyTorch, Python random)
```

---

## Summary Table

| Property | Value | Type |
|----------|-------|------|
| **State Space** | â„^{4N+3} | Continuous, bounded |
| **Action Space** | {0,...,N-1} | Discrete, finite |
| **Masking** | M: S â†’ {0,1}^N | State-dependent |
| **Transition** | P(s'|s,a) | Partially stochastic |
| **Reward** | R: SÃ—AÃ—S â†’ â„ | Bounded, sparse+dense |
| **Episode Length** | T âˆˆ [N, 2N] | Variable |
| **Discount** | Î³ = 0.99 | Standard |
| **Observability** | Full (episodic) | Complete state |
| **Stochasticity** | Execution outcomes | Bernoulli(p_CVSS) |
| **Complexity** | O(N) per step | Linear in tasks |

---

This specification provides the complete formal mathematical and technical foundation for the PPO pentest training environment.
