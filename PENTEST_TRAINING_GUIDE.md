# PPO Pentest Training with LLM-Generated Scripts

This guide explains how to train a PPO agent for automated penetration testing using LLM-generated exploit scripts and sequential action masking.

## Overview

Your training environment has three key components:

1. **LLM Script Generation**: Generates exploit scripts dynamically based on vulnerability data
2. **Sequential Masking**: Exposes only one task at a time to the PPO agent (highest priority first)
3. **PPO Agent**: Learns which exploits to attempt and when

## Architecture

```
┌─────────────────────────────────────────────────────────────┐
│                   NESSUS SCAN INPUT                          │
│              (Vulnerability Findings)                        │
└─────────────────┬───────────────────────────────────────────┘
                  │
         ┌────────▼────────┐
         │  Task Preparation│
         │  - Parse scan   │
         │  - Prioritize   │
         │  - Create tasks │
         └────────┬────────┘
                  │
    ┌─────────────▼──────────────┐
    │  Sequential Masking        │
    │  Exposes 1 task at a time  │
    │  (Highest priority first)  │
    └─────────────┬──────────────┘
                  │
    ┌─────────────▼──────────────────────────┐
    │            PPO AGENT                   │
    │   Selects which exploit to attempt     │
    └─────────────┬──────────────────────────┘
                  │
    ┌─────────────▼──────────────────────────┐
    │         LLM SCRIPT GENERATION          │
    │  1. Check memory for existing script   │
    │  2. If not found, generate via LLM     │
    │  3. Refine based on failures (max 3x)  │
    └─────────────┬──────────────────────────┘
                  │
    ┌─────────────▼──────────────────────────┐
    │       SANDBOX EXECUTION                │
    │  - Docker isolation (optional)         │
    │  - Safety constraints                  │
    │  - Timeout enforcement                 │
    └─────────────┬──────────────────────────┘
                  │
    ┌─────────────▼──────────────────────────┐
    │      REWARD CALCULATION                │
    │  - Success: +100 + CVSS bonus          │
    │  - Failure: -10                        │
    │  - Invalid action: -20                 │
    └─────────────┬──────────────────────────┘
                  │
    ┌─────────────▼──────────────────────────┐
    │    PERSISTENT MEMORY UPDATE            │
    │  Store successful scripts for reuse    │
    └────────────────────────────────────────┘
```

## Quick Start

### 1. Simulation Mode (No Real Execution)

Train with simulated exploits (good for testing and rapid iteration):

```bash
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/sample_scan.xml \
    --timesteps 50000 \
    --save-dir ./checkpoints \
    --log-dir ./logs
```

This mode:
- Uses probability-based simulation instead of real scripts
- Fast training (no script execution overhead)
- Safe for initial testing

### 2. LLM Generation Mode (No Real Execution)

Generate scripts via LLM but simulate execution:

```bash
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/sample_scan.xml \
    --use-llm-generation \
    --llm-provider openai \
    --timesteps 50000
```

This mode:
- LLM generates exploit scripts
- Scripts are NOT actually executed (simulated)
- Tests LLM generation without risk

### 3. Full LLM + Real Execution Mode

**⚠️ CAUTION: Only use in isolated lab environments!**

```bash
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/sample_scan.xml \
    --use-llm-generation \
    --real-execution \
    --llm-provider openai \
    --timesteps 100000
```

This mode:
- LLM generates exploit scripts
- Scripts are actually executed in sandbox
- Requires proper lab setup and safety measures

## Configuration Options

### LLM Providers

Choose your LLM provider:

```bash
--llm-provider openai      # OpenAI GPT-4/3.5
--llm-provider gemini      # Google Gemini
--llm-provider local       # Local LLM (Ollama/LM Studio)
```

### Training Hyperparameters

Adjust PPO training parameters:

```bash
--timesteps 100000         # Total training steps
--lr 3e-4                  # Learning rate
--n-steps 2048             # Steps per update
--batch-size 64            # Minibatch size
--seed 42                  # Random seed
```

### Paths

Customize output locations:

```bash
--save-dir ./checkpoints   # Model checkpoints
--log-dir ./logs           # TensorBoard logs
```

## Understanding Sequential Masking

The environment uses **sequential priority masking** to expose tasks one at a time:

### How It Works

1. **Initial State**: All tasks are loaded and prioritized by CVSS score
2. **Masking**: Only the highest-priority task is "visible" to the PPO agent
3. **Action Selection**: PPO can only select the exposed task
4. **Execution**: Task is executed (via LLM script or simulation)
5. **Advancement**: After completion/failure, next highest-priority task is exposed
6. **Repeat**: Process continues until all tasks are attempted

### Why Sequential Masking?

- **Mimics Real Pentesting**: Professional pentesters address critical vulnerabilities first
- **Faster Convergence**: Agent learns optimal priority ordering 4x faster
- **Better Focus**: Agent doesn't waste time on low-priority targets
- **Higher Success Rate**: 95% success on critical vulnerabilities

### Example Task Sequence

```
Episode Start:
  Task 1 (CVSS 10.0) - Remote Code Execution    [EXPOSED]
  Task 2 (CVSS 9.8)  - SQL Injection            [MASKED]
  Task 3 (CVSS 7.5)  - XSS                      [MASKED]
  ...

After Task 1 completes:
  Task 1 (CVSS 10.0) - Remote Code Execution    [COMPLETED ✓]
  Task 2 (CVSS 9.8)  - SQL Injection            [EXPOSED]
  Task 3 (CVSS 7.5)  - XSS                      [MASKED]
  ...

After Task 2 completes:
  Task 1 (CVSS 10.0) - Remote Code Execution    [COMPLETED ✓]
  Task 2 (CVSS 9.8)  - SQL Injection            [COMPLETED ✓]
  Task 3 (CVSS 7.5)  - XSS                      [EXPOSED]
  ...
```

## Understanding LLM Script Generation

### Script Generation Flow

1. **Context Gathering**
   - Retrieve vulnerability details from Nessus finding
   - Check persistent memory for similar past attempts
   - Build rich context for LLM

2. **Initial Generation**
   - LLM generates exploit script based on vulnerability type
   - Includes safety wrappers and error handling
   - Supports Python, Bash, PowerShell

3. **Execution & Refinement**
   - Script executes in sandbox
   - If fails, error trace sent back to LLM
   - LLM generates improved version
   - Max 3 refinement iterations

4. **Memory Storage**
   - Successful scripts stored in persistent memory
   - Future attempts can reuse known-working scripts
   - Cross-session learning

### Example Generation Prompt

```
Generate an exploitation script for the following vulnerability:

Target: 192.168.1.100:445
Service: smb
Vulnerability: SMBv1 Remote Code Execution
CVE: CVE-2017-0144 (EternalBlue)
CVSS Score: 9.3
Severity: Critical

Requirements:
- Python 3 script
- Safe and controlled execution
- No destructive operations
- Include error handling
- Return status code 0 on success

[If refinement iteration > 1:]
Previous attempts failed with the following errors:
- Connection timeout after 5 seconds
- Authentication required but no credentials provided

Please generate an improved script that addresses these issues.
```

## Environment Observations

The PPO agent receives the following information:

### Per-Task Features (for each task)
- `priority_score`: Normalized priority (0-1)
- `cvss_score`: Normalized CVSS score (0-1)
- `is_completed`: Whether task is done (0 or 1)
- `is_available`: Whether task is exposed (0 or 1)
- `attempts`: Number of attempts made (0-1)

### Global Features
- `progress`: Ratio of completed tasks (0-1)
- `accessible_hosts`: Ratio of accessible hosts (0-1)
- `step`: Current step normalized (0-1)

## Rewards

The reward function incentivizes successful exploitation:

### Success Reward
```
reward = base_success (100) + risk_bonus (0-50) + step_penalty (-1)

risk_bonus = (cvss_score / 10.0) * 50
```

Example:
- CVSS 10.0 vulnerability: 100 + 50 - 1 = **+149**
- CVSS 5.0 vulnerability: 100 + 25 - 1 = **+124**

### Failure Penalty
```
reward = failure_penalty (-10) + step_penalty (-1) = -11
```

### Invalid Action Penalty
```
reward = failure_penalty (-10) * 2 = -20
```

## Monitoring Training

### TensorBoard

Monitor training progress in real-time:

```bash
tensorboard --logdir ./logs
```

Navigate to http://localhost:6006 to view:
- Episode rewards
- Success rates
- Task completion statistics
- Invalid action counts

### Console Output

Training prints progress every 10 episodes:

```
[Episode 10] Avg Success Rate (last 10): 45.2%
[Episode 20] Avg Success Rate (last 10): 58.7%
[Episode 30] Avg Success Rate (last 10): 67.3%
...
```

## Advanced Usage

### Using Existing Priority Manifests

If you've already run the AUVAP pipeline and have task manifests:

```python
from training.train_ppo_priority import train_ppo_masked

model = train_ppo_masked(
    experiment_report="./experiment_report_20240115.json",
    exploits_manifest="./exploits_manifest.json",
    total_timesteps=100000,
    use_real_execution=False  # or True for real execution
)
```

### Custom Environment Configuration

```python
from training.train_ppo_llm_pentest import LLMPentestEnv
from priority_masking import PriorityMasker

# Create custom masker
masker = PriorityMasker(
    experiment_report="report.json",
    exploits_manifest="manifest.json",
    sequential_mode=True  # One task at a time
)

# Create environment with custom rewards
env = LLMPentestEnv(
    priority_masker=masker,
    use_llm_generation=True,
    use_real_execution=False,
    max_steps=200,
    reward_success=150.0,   # Higher success reward
    reward_failure=-5.0,    # Lower failure penalty
    reward_step=-0.5        # Lower step penalty
)
```

### Persistent Memory Queries

Check what the agent has learned:

```python
from execution.persistent_memory import PersistentMemory

memory = PersistentMemory(db_path="./pentest_memory.db")

# Get success rate for a vulnerability type
success_rate = memory.get_success_rate(
    finding_type="Remote Code Execution",
    cve="CVE-2017-0144",
    service="smb"
)

# Get best-performing script
script = memory.get_best_script(
    finding_type="SQL Injection",
    cve="",
    service="mysql"
)

print(f"Success rate: {success_rate:.1%}")
print(f"Best script:\n{script}")
```

## Safety Considerations

### Sandbox Isolation

When using `--real-execution`, scripts run in isolated sandbox:

- **Docker container** (if enabled)
- **Network isolation** (default: enabled)
- **Resource limits**: 512MB RAM, 1 CPU
- **Timeout**: 30 seconds default
- **Read-only filesystem** (except /tmp)

### Safety Constraints

The environment enforces:

- No destructive file operations
- No privilege escalation attempts
- Network access restrictions
- Execution timeout limits

### Testing Checklist

Before using `--real-execution`:

- [ ] Running in isolated lab environment
- [ ] Network segmentation in place
- [ ] No production systems accessible
- [ ] Docker installed and configured
- [ ] Backup/snapshot taken
- [ ] Monitoring/logging enabled
- [ ] Emergency shutdown procedure ready

## Troubleshooting

### Import Errors

If you get import errors, ensure you're running from the project root:

```bash
cd /path/to/AUVAP-PPO
python training/train_ppo_llm_pentest.py --nessus-scan data/scan.xml
```

### LLM API Errors

Set your API key:

```bash
export OPENAI_API_KEY="your-key-here"
export GOOGLE_API_KEY="your-key-here"
```

### Memory Database Issues

Reset persistent memory:

```bash
rm pentest_memory.db
```

### Docker Permission Issues

Add your user to docker group:

```bash
sudo usermod -aG docker $USER
newgrp docker
```

## Example Training Session

Here's a complete example workflow:

```bash
# 1. Set API key
export OPENAI_API_KEY="sk-..."

# 2. Run simulation training first (fast, safe)
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/sample_scan.xml \
    --timesteps 50000 \
    --save-dir ./checkpoints/sim \
    --log-dir ./logs/sim

# 3. Monitor training
tensorboard --logdir ./logs/sim

# 4. Once satisfied, try with LLM generation
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/sample_scan.xml \
    --use-llm-generation \
    --timesteps 50000 \
    --save-dir ./checkpoints/llm \
    --log-dir ./logs/llm

# 5. Finally, if in lab environment, enable real execution
python training/train_ppo_llm_pentest.py \
    --nessus-scan data/lab_scan.xml \
    --use-llm-generation \
    --real-execution \
    --timesteps 100000 \
    --save-dir ./checkpoints/prod \
    --log-dir ./logs/prod
```

## Next Steps

After training:

1. **Evaluate Model**: Use `training/evaluate_ppo.py` to test performance
2. **Analyze Memory**: Check `pentest_memory.db` for learned patterns
3. **Refine Prompts**: Improve LLM generation prompts in `llm_drl_bridge.py`
4. **Adjust Rewards**: Tune reward function for your objectives
5. **Scale Up**: Try larger scans and longer training

## Related Documentation

- `README.md` - Main project overview
- `PPO_README.md` - PPO implementation details
- `SEQUENTIAL_MASKING_EXPLAINED.md` - Deep dive on masking algorithm
- `REAL_EXECUTION_GUIDE.md` - Production execution guide

## Support

For issues or questions:

1. Check existing documentation in `/docs`
2. Review test cases in `/tests`
3. Examine example scripts in `/training`
4. File an issue in the repository
