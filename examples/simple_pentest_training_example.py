#!/usr/bin/env python3
"""
Simple Pentest Training Example

This script demonstrates the complete workflow:
1. Load vulnerability findings from Nessus scan
2. Create sequential masking environment
3. Train PPO agent with LLM-generated scripts
4. Evaluate results

Run this example to see how everything works together.
"""

import os
import sys
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv

# Import AUVAP components
from parser import VAFinding
from priority_masking import PriorityMasker
from task_manager import TaskManager
from training.train_ppo_llm_pentest import LLMPentestEnv, ProgressCallback


def create_sample_findings():
    """
    Create sample vulnerability findings for demonstration.

    In production, these would come from parse_nessus_xml().
    """
    findings = [
        VAFinding(
            plugin_id="97833",
            plugin_name="MS17-010: Security Update for Microsoft Windows SMB Server (4013389)",
            severity="Critical",
            cvss_base_score=10.0,
            synopsis="Remote code execution vulnerability in SMBv1",
            description="The remote Windows host is affected by a critical RCE vulnerability (EternalBlue)",
            protocol="tcp",
            port="445",
            host="192.168.1.10"
        ),
        VAFinding(
            plugin_id="42873",
            plugin_name="SSL Certificate Cannot Be Trusted",
            severity="Medium",
            cvss_base_score=6.4,
            synopsis="SSL certificate validation issue",
            description="The SSL certificate cannot be trusted.",
            protocol="tcp",
            port="443",
            host="192.168.1.10"
        ),
        VAFinding(
            plugin_id="45590",
            plugin_name="MySQL Unpassworded Root Account",
            severity="High",
            cvss_base_score=9.0,
            synopsis="MySQL root account has no password",
            description="The remote MySQL server has a root account with no password.",
            protocol="tcp",
            port="3306",
            host="192.168.1.20"
        ),
        VAFinding(
            plugin_id="11219",
            plugin_name="Nessus SYN scanner",
            severity="Info",
            cvss_base_score=0.0,
            synopsis="Port scan results",
            description="This plugin is a SYN 'half-open' port scanner.",
            protocol="tcp",
            port="22",
            host="192.168.1.20"
        ),
        VAFinding(
            plugin_id="22964",
            plugin_name="SSH Server CBC Mode Ciphers Enabled",
            severity="Low",
            cvss_base_score=2.6,
            synopsis="Weak SSH cipher configuration",
            description="The SSH server is configured to use CBC mode ciphers.",
            protocol="tcp",
            port="22",
            host="192.168.1.30"
        ),
    ]

    return findings


def create_sample_tasks(findings):
    """Create exploit tasks from findings"""
    task_manager = TaskManager()

    for finding in findings:
        # Only create tasks for exploitable vulnerabilities
        if finding.cvss_base_score and finding.cvss_base_score >= 2.0:
            task = task_manager.create_task_from_finding(finding)

    print(f"✓ Created {len(task_manager.tasks)} exploit tasks")

    return task_manager


def save_sample_manifests(task_manager, output_dir):
    """Save task manifests to JSON"""
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Save exploits manifest
    manifest_path = output_dir / "exploits_manifest.json"
    task_manager.save_tasks(str(manifest_path))

    # Create dummy experiment report
    experiment_path = output_dir / "experiment_report.json"
    experiment_path.write_text("{}")

    print(f"✓ Saved manifests to {output_dir}")

    return str(experiment_path), str(manifest_path)


def train_simple_example(
    timesteps=10000,
    use_llm=False,
    use_real_exec=False,
    verbose=True
):
    """
    Run simple training example.

    Args:
        timesteps: Number of training steps
        use_llm: Use LLM generation (requires API key)
        use_real_exec: Execute real scripts (requires lab setup)
        verbose: Print detailed logs
    """
    print("\n" + "="*70)
    print("SIMPLE PENTEST TRAINING EXAMPLE")
    print("="*70)
    print(f"Mode: {'LLM+Real' if use_llm and use_real_exec else 'LLM+Sim' if use_llm else 'Simulation'}")
    print(f"Timesteps: {timesteps}")
    print("="*70 + "\n")

    # Step 1: Create sample data
    print("[1/5] Creating sample vulnerability findings...")
    findings = create_sample_findings()
    print(f"  ✓ Created {len(findings)} findings")

    for i, finding in enumerate(findings, 1):
        print(f"    {i}. {finding.plugin_name} (CVSS: {finding.cvss_base_score})")

    # Step 2: Create tasks
    print(f"\n[2/5] Creating exploit tasks...")
    task_manager = create_sample_tasks(findings)

    # Step 3: Save manifests
    print(f"\n[3/5] Saving task manifests...")
    output_dir = Path("./examples/sample_data")
    experiment_report, exploits_manifest = save_sample_manifests(
        task_manager, output_dir
    )

    # Step 4: Initialize environment
    print(f"\n[4/5] Initializing training environment...")

    # Create priority masker (sequential mode)
    masker = PriorityMasker(
        experiment_report,
        exploits_manifest,
        sequential_mode=True
    )

    print(f"\n  Sequential Masking Status:")
    masker.print_status()

    # LLM bridge (only if enabled)
    llm_bridge = None
    if use_llm:
        from execution.sandbox_executor import SandboxExecutor
        from execution.persistent_memory import PersistentMemory
        from execution.llm_drl_bridge import LLMDRLBridge

        sandbox = SandboxExecutor(use_docker=use_real_exec, timeout=30)
        memory = PersistentMemory(db_path="./examples/sample_memory.db")

        llm_bridge = LLMDRLBridge(
            sandbox_executor=sandbox,
            persistent_memory=memory,
            llm_provider="openai",
            max_refinement_iterations=3,
            verbose=verbose
        )
        print(f"  ✓ LLM bridge initialized")

    # Create environment
    def make_env():
        env = LLMPentestEnv(
            priority_masker=masker,
            llm_bridge=llm_bridge,
            use_llm_generation=use_llm,
            use_real_execution=use_real_exec,
            max_steps=len(masker.tasks) * 3,
            verbose=verbose
        )

        # Map findings to tasks
        findings_map = {}
        for task, finding in zip(task_manager.tasks, findings):
            if task is not None:
                findings_map[task.task_id] = finding
        env.set_findings_map(findings_map)

        return env

    env = DummyVecEnv([make_env])
    print(f"  ✓ Environment created")

    # Step 5: Train PPO
    print(f"\n[5/5] Training PPO agent...")

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=512,  # Smaller for quick example
        batch_size=32,
        n_epochs=5,
        gamma=0.99,
        verbose=1,
        seed=42
    )

    print(f"  ✓ PPO initialized")
    print(f"  - Observation space: {env.observation_space}")
    print(f"  - Action space: {env.action_space}")
    print(f"  - Parameters: {sum(p.numel() for p in model.policy.parameters()):,}")

    print(f"\n{'='*70}")
    print("TRAINING STARTED")
    print('='*70 + "\n")

    # Callback for tracking progress
    callback = ProgressCallback(verbose=1)

    # Train
    model.learn(
        total_timesteps=timesteps,
        callback=callback,
        log_interval=10,
        progress_bar=True
    )

    # Results
    print(f"\n{'='*70}")
    print("TRAINING COMPLETE")
    print('='*70)

    if callback.episode_success_rates:
        avg_success = np.mean(callback.episode_success_rates)
        recent_success = np.mean(callback.episode_success_rates[-10:]) if len(callback.episode_success_rates) >= 10 else avg_success

        print(f"\nResults:")
        print(f"  Episodes completed: {len(callback.episode_success_rates)}")
        print(f"  Average success rate: {avg_success:.1%}")
        print(f"  Recent success rate (last 10): {recent_success:.1%}")

        # Show learning curve
        print(f"\n  Learning curve:")
        for i in range(0, len(callback.episode_success_rates), max(1, len(callback.episode_success_rates) // 10)):
            rate = callback.episode_success_rates[i]
            bar_length = int(rate * 50)
            bar = "█" * bar_length + "░" * (50 - bar_length)
            print(f"    Episode {i+1:3d}: {bar} {rate:.1%}")

    print(f"\n  Final masking status:")
    masker.print_status()

    env.close()

    # Save model
    save_path = Path("./examples/sample_model.zip")
    model.save(save_path)
    print(f"\n✓ Model saved to: {save_path}")

    return model, callback


def evaluate_model(model_path="./examples/sample_model.zip", num_episodes=5):
    """
    Evaluate a trained model.

    Args:
        model_path: Path to saved model
        num_episodes: Number of episodes to run
    """
    print("\n" + "="*70)
    print("MODEL EVALUATION")
    print("="*70 + "\n")

    # Load model
    print(f"[1/3] Loading model from {model_path}...")
    model = PPO.load(model_path)
    print(f"  ✓ Model loaded")

    # Recreate environment
    print(f"\n[2/3] Creating evaluation environment...")
    findings = create_sample_findings()
    task_manager = create_sample_tasks(findings)

    output_dir = Path("./examples/sample_data")
    experiment_report, exploits_manifest = save_sample_manifests(
        task_manager, output_dir
    )

    masker = PriorityMasker(experiment_report, exploits_manifest, sequential_mode=True)

    env = LLMPentestEnv(
        priority_masker=masker,
        use_llm_generation=False,
        use_real_execution=False,
        verbose=False
    )

    findings_map = {}
    for task, finding in zip(task_manager.tasks, findings):
        if task is not None:
            findings_map[task.task_id] = finding
    env.set_findings_map(findings_map)

    print(f"  ✓ Environment created")

    # Evaluate
    print(f"\n[3/3] Running {num_episodes} evaluation episodes...")

    episode_rewards = []
    episode_success_rates = []

    for ep in range(num_episodes):
        obs, info = env.reset()
        done = False
        episode_reward = 0
        successes = 0
        total = 0

        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(int(action))

            episode_reward += reward
            done = terminated or truncated

            if info.get('task_completed'):
                total += 1
                if info.get('success'):
                    successes += 1

        success_rate = successes / total if total > 0 else 0
        episode_rewards.append(episode_reward)
        episode_success_rates.append(success_rate)

        print(f"  Episode {ep+1}: Reward={episode_reward:.1f}, Success={success_rate:.1%}")

    # Summary
    print(f"\n{'='*70}")
    print("EVALUATION RESULTS")
    print('='*70)
    print(f"  Mean reward: {np.mean(episode_rewards):.1f} ± {np.std(episode_rewards):.1f}")
    print(f"  Mean success rate: {np.mean(episode_success_rates):.1%}")
    print(f"  Best episode: {np.max(episode_rewards):.1f}")
    print(f"  Worst episode: {np.min(episode_rewards):.1f}")

    env.close()


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Simple pentest training example")

    parser.add_argument("--mode", type=str, default="train",
                       choices=["train", "eval"],
                       help="Run mode: train or eval")
    parser.add_argument("--timesteps", type=int, default=10000,
                       help="Training timesteps")
    parser.add_argument("--use-llm", action="store_true",
                       help="Use LLM generation (requires API key)")
    parser.add_argument("--real-exec", action="store_true",
                       help="Execute real scripts (requires lab)")
    parser.add_argument("--quiet", action="store_true",
                       help="Reduce verbosity")

    args = parser.parse_args()

    if args.mode == "train":
        model, callback = train_simple_example(
            timesteps=args.timesteps,
            use_llm=args.use_llm,
            use_real_exec=args.real_exec,
            verbose=not args.quiet
        )

        # Optionally evaluate after training
        print("\n" + "="*70)
        response = input("Evaluate trained model? [y/N]: ")
        if response.lower() == 'y':
            evaluate_model()

    elif args.mode == "eval":
        evaluate_model()


if __name__ == "__main__":
    main()
