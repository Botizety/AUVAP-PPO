#!/usr/bin/env python3
"""
exploit_generator.py - LLM-Based Exploit Script Generator

This module generates safe, context-aware penetration testing scripts using LLM.
It reads feasible vulnerabilities from the AUVAP pipeline and creates ethical,
validated Python exploit scripts for proof-of-concept testing.

IMPORTANT SECURITY NOTICE:
- All generated scripts are for AUTHORIZED TESTING ONLY
- Scripts include safety constraints and scope validation
- No destructive actions are included
- Proper error handling and logging is enforced
- Scripts must be reviewed before execution

NO UNAUTHORIZED TESTING. ETHICAL USE ONLY.
"""

import ast
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Optional


# ============================================================================
# LLM Provider Utilities
# ============================================================================

def _normalize_model_alias(provider: str, model: Optional[str]) -> Optional[str]:
    """Normalize common model aliases to canonical names per provider."""
    if not model:
        return model
    name = model.strip().lower().replace("_", "-").replace(" ", "-")
    if provider == 'openai' and name in {"gpt5-thinking", "gpt-5thinking", "gpt5thinking"}:
        return "gpt-5-thinking"
    return model


def _is_openai_reasoning_model(model: Optional[str]) -> bool:
    """Check if model is an OpenAI reasoning model."""
    if not model:
        return False
    m = model.lower()
    return m.startswith("gpt-5") or m.startswith("o4") or m.startswith("o3") or ("reason" in m)


def _call_llm_api(prompt: str, provider: str, api_key: str, model: str,
                  base_url: str = "https://api.openai.com/v1",
                  reasoning_effort: str = "high", verbosity: str = "low") -> str:
    """
    Unified LLM API caller supporting OpenAI, Gemini, and compatible providers.

    Args:
        prompt: The prompt to send
        provider: Provider type (openai, gemini, github, local)
        api_key: API key for authentication
        model: Model name
        base_url: API base URL (for OpenAI-compatible APIs)
        reasoning_effort: Effort level for reasoning models
        verbosity: Verbosity level for reasoning models

    Returns:
        Generated text response
    """
    if provider == "gemini":
        try:
            from google import genai  # type: ignore[import]
            from google.genai import types  # type: ignore[import]
        except ImportError as exc:
            raise ImportError("google-genai package not installed. Run: pip install google-genai") from exc

        client = genai.Client(api_key=api_key)
        response = client.models.generate_content(
            model=model,
            contents=prompt,
            config=types.GenerateContentConfig(temperature=0.2, max_output_tokens=1600)
        )
        return response.text.strip()

    # OpenAI and compatible providers
    try:
        from openai import OpenAI
    except ImportError as exc:
        raise ImportError("openai package not installed. Run: pip install openai") from exc

    client = OpenAI(api_key=api_key, base_url=base_url)

    # Use Responses API for reasoning models
    if _is_openai_reasoning_model(model):
        resp = client.responses.create(
            model=model,
            input=prompt,
            reasoning={"effort": reasoning_effort},  # type: ignore[arg-type]
            text={"verbosity": verbosity}  # type: ignore[arg-type]
        )
        text = getattr(resp, "output_text", None)
        if isinstance(text, str) and text.strip():
            return text.strip()

        # Fallback: concatenate text parts
        try:
            parts = []
            output = getattr(resp, "output", None)
            if output:
                for item in output:
                    content = getattr(item, "content", None)
                    if content:
                        for c in content:
                            if getattr(c, "type", "") in ("output_text", "text") and getattr(c, "text", ""):
                                parts.append(c.text)
            if parts:
                return "\n".join(parts).strip()
        except Exception:
            pass
        return ""

    # Standard chat completions
    response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a senior penetration tester generating safe proof-of-concept scripts. Respond with Python code only."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2,
        max_tokens=1500
    )
    return (response.choices[0].message.content or "").strip()


def _detect_provider_and_key(provider: str) -> tuple[str, Optional[str]]:
    """
    Detect LLM provider and get API key from environment.

    Returns:
        Tuple of (provider_name, api_key)
    """
    if provider != "auto":
        # Use specified provider
        if provider == "github":
            return ("github", os.environ.get('GITHUB_TOKEN'))
        elif provider == "gemini":
            return ("gemini", os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY'))
        elif provider == "openai":
            return ("openai", os.environ.get('OPENAI_API_KEY'))
        elif provider == "local":
            return ("local", None)

    # Auto-detect from environment
    if os.environ.get('GITHUB_TOKEN'):
        return ("github", os.environ['GITHUB_TOKEN'])
    elif os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY'):
        return ("gemini", os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY'))
    elif os.environ.get('OPENAI_API_KEY'):
        return ("openai", os.environ['OPENAI_API_KEY'])
    else:
        return ("local", None)


def _get_provider_models(provider: str, model: Optional[str]) -> list[str]:
    """Get model candidates for a provider."""
    if model:
        return [model]

    models = {
        "github": ["gpt-4o-mini", "gpt-4o"],
        "gemini": ["gemini-2.0-flash-exp", "gemini-1.5-flash"],
        "openai": ["gpt-5", "gpt-4o-mini", "gpt-5-nano"],
        "local": ["deepseek-r1:14b", "qwen3:14b"]
    }
    return models.get(provider, ["gpt-4o-mini"])


def _get_base_url(provider: str) -> str:
    """Get base URL for provider."""
    if provider == "github":
        return "https://models.inference.ai.azure.com"
    elif provider == "local":
        return os.environ.get('LOCAL_OPENAI_BASE_URL', 'http://localhost:11434/v1')
    return "https://api.openai.com/v1"


# ============================================================================
# Language Selection
# ============================================================================

def choose_language_for_vuln(vuln: dict) -> str:
    """
    Determine the best scripting language for a vulnerability.

    Returns: 'bash', 'powershell', 'python', or 'ruby'
    """
    service = (vuln.get('service') or '').lower()
    title = (vuln.get('title') or '').lower()
    os_type = (vuln.get('os') or '').lower()
    cve = (vuln.get('cve') or '').lower()
    port = vuln.get('port', 0)

    # Windows-specific
    if 'windows' in os_type or 'microsoft' in title:
        return 'powershell'

    if service in ['smb', 'microsoft-ds', 'netbios-ssn', 'msrpc', 'rdp', 'ms-wbt-server', 'winrm'] or 'iis' in service or 'iis' in title:
        return 'powershell'

    # Known CVE mappings
    cve_lang_map = {
        'cve-2017-0144': 'powershell', 'cve-2017-0145': 'powershell',
        'cve-2014-6271': 'bash', 'cve-2014-7169': 'bash',
        'cve-2017-7269': 'powershell', 'cve-2019-0708': 'powershell',
        'cve-2020-0796': 'powershell'
    }
    if cve in cve_lang_map:
        return cve_lang_map[cve]

    # Unix/Linux services
    if service in ['ssh', 'ftp', 'telnet', 'smtp', 'pop3', 'imap', 'nntp', 'finger']:
        return 'bash'

    # Web services
    web_services = ['http', 'https', 'apache', 'tomcat', 'nginx', 'jenkins', 'wordpress', 'joomla']
    if service in web_services or any(ws in title for ws in ['http', 'web', 'apache', 'tomcat', 'nginx']):
        return 'python'

    # Databases
    if any(db in service for db in ['postgresql', 'mysql', 'mariadb', 'mongodb', 'redis', 'oracle', 'mssql']):
        return 'python'

    # Keyword-based
    if 'shellshock' in title or 'bash' in title or ('command injection' in title and 'windows' not in os_type):
        return 'bash'

    if 'buffer overflow' in title or 'memory corruption' in title:
        return 'python'

    # Port-based
    if port in [445, 139, 135, 3389]:
        return 'powershell'
    if port in [22, 21, 23, 25]:
        return 'bash'

    return 'python'


# ============================================================================
# Prompt Building
# ============================================================================

def build_exploit_prompt(vuln: dict, target_language: str) -> str:
    """Build LLM prompt for generating safe exploit script."""
    lang_instructions = {
        'bash': "Generate a Bash script (.sh) using standard Unix tools (curl, nc, ssh, openssl, etc.).\n- Start with #!/bin/bash\n- Use bash error handling (set -e, trap, || exit)\n- Use 'timeout' command for timeouts\n- Output results to stdout",
        'powershell': "Generate a PowerShell script (.ps1) using native cmdlets.\n- Use Invoke-WebRequest, Invoke-Command, Test-NetConnection, etc.\n- Use try-catch-finally for error handling\n- Set $ErrorActionPreference = 'Stop'\n- Use -TimeoutSec for timeouts\n- Return PSCustomObject with Success, Evidence, Logs properties",
        'python': "Generate a Python 3 script (.py) using standard libraries.\n- Use requests, socket, paramiko, ssl, http.client when needed\n- Use try-except-finally for error handling\n- Include function: def exploit(target_host: str, target_port: int, timeout: int = 10) -> dict\n- Return dict with keys: success (bool), evidence (str), logs (list)",
        'ruby': "Generate a Ruby script (.rb) using standard libraries or Metasploit.\n- Use Net::HTTP, Socket, OpenSSL when needed\n- Use begin-rescue-ensure for error handling\n- Include method: def exploit(target_host, target_port, timeout = 10)\n- Return hash with keys: success, evidence, logs"
    }

    lang_name = target_language.capitalize()
    return f"""Generate a safe, ethical penetration testing script in {lang_name.upper()} for this vulnerability:

Target: {vuln.get('host_ip', 'unknown')}:{vuln.get('port', 0)}
Service: {vuln.get('service', 'unknown')}
Operating System: {vuln.get('os', 'Unknown')}
Vulnerability: {vuln.get('title', 'Unknown Vulnerability')}
CVE: {vuln.get('cve', 'N/A')}
Vulnerable Component: {vuln.get('vuln_component', 'unknown')}

Description: {vuln.get('description', 'No description available')}

Exploit Guidance: {vuln.get('exploit_notes', '')}

LANGUAGE REQUIREMENTS:
{lang_instructions.get(target_language, lang_instructions['python'])}

SAFETY REQUIREMENTS:
- Include timeout: 10 seconds maximum
- Max attempts: 3
- Delay between attempts: 2 seconds
- Validate target is in scope before ANY network activity
- Add detailed logging/output for each step
- DO NOT include destructive actions (file deletion, service shutdown, etc.)
- Focus on proof-of-concept validation ONLY (verify vulnerability exists)

OUTPUT REQUIREMENTS:
At the TOP of your response, specify the language on the first line as a comment:
# Language: {lang_name}

Then provide ONLY the complete code with no markdown formatting, no explanations, no preamble.
Start directly with language declaration (# Language: {lang_name}) followed by code."""


# ============================================================================
# Script Generation
# ============================================================================

def generate_exploit_script(vuln: dict, provider: str = "auto", model: Optional[str] = None) -> str:
    """
    Generate exploit script using LLM.

    Args:
        vuln: Vulnerability dictionary
        provider: LLM provider ("auto", "github", "gemini", "openai", "local")
        model: Optional model name override

    Returns:
        Generated exploit script code in appropriate language
    """
    target_language = choose_language_for_vuln(vuln)
    print(f"      Chosen language: {target_language}", file=sys.stderr)

    prompt = build_exploit_prompt(vuln, target_language)

    # Detect provider and get API key
    actual_provider, api_key = _detect_provider_and_key(provider)

    if not api_key and actual_provider != "local":
        raise RuntimeError(f"{actual_provider.upper()}_TOKEN/API_KEY not set")

    print(f"      Generating exploit using {actual_provider}...", file=sys.stderr)

    # Try models with fallback
    model_candidates = _get_provider_models(actual_provider, model)
    base_url = _get_base_url(actual_provider)
    last_error: Optional[Exception] = None

    for candidate in model_candidates:
        try:
            print(f"      -> Attempting model: {candidate}", file=sys.stderr)
            script = _call_llm_api(prompt, actual_provider, api_key or "local", candidate, base_url)
            break
        except Exception as err:
            last_error = err
            error_text = str(err)

            # Check if transient error
            if any(x in error_text for x in ["does not exist", "404", "not found", "429", "quota", "rate limit", "exceeded your current quota"]):
                print(f"      ⚠️  Model {candidate} unavailable, trying fallback...", file=sys.stderr)
                if "429" in error_text or "quota" in error_text.lower() or "rate limit" in error_text.lower():
                    time.sleep(3)
                continue
            raise
    else:
        assert last_error is not None
        raise last_error

    # Clean up response
    code = str(script).strip()

    # Remove markdown code blocks
    if code.startswith('```'):
        lines = code.split('\n')
        start_idx, end_idx = 0, len(lines)
        for i, line in enumerate(lines):
            if line.startswith('```'):
                if start_idx == 0:
                    start_idx = i + 1
                else:
                    end_idx = i
                    break
        code = '\n'.join(lines[start_idx:end_idx])

    return code.strip()


# ============================================================================
# Script Validation
# ============================================================================

def validate_script_safety(script_code: str) -> tuple[bool, list[str]]:
    """
    Validate generated script for safety issues.

    Returns:
        Tuple of (is_safe: bool, warnings: list[str])
    """
    warnings = []
    first_line = script_code.split('\n')[0].lower() if script_code else ''

    # Detect language
    is_python = '# language: python' in first_line or script_code.startswith('#!/usr/bin/env python') or script_code.startswith('#!/usr/bin/python')
    is_bash = '# language: bash' in first_line or script_code.startswith('#!/bin/bash')
    is_powershell = '# language: powershell' in first_line or script_code.startswith('#Requires')

    # Check for hardcoded credentials
    cred_patterns = [r'password\s*=\s*["\'][^"\']+["\']', r'passwd\s*=\s*["\'][^"\']+["\']', r'api_key\s*=\s*["\'][^"\']+["\']',
                     r'secret\s*=\s*["\'][^"\']+["\']', r'token\s*=\s*["\'][a-zA-Z0-9]{20,}["\']']
    for pattern in cred_patterns:
        if re.search(pattern, script_code, re.IGNORECASE):
            warnings.append(f"Potential hardcoded credential found: {pattern}")

    # Check timeout
    if 'timeout' not in script_code.lower():
        warnings.append("No timeout mechanism detected")

    # Check error handling
    if is_python and ('try:' not in script_code or 'except' not in script_code):
        warnings.append("Insufficient error handling (missing try-except)")
    elif is_bash and not any(x in script_code for x in ['set -e', 'trap', '|| exit']):
        warnings.append("Insufficient error handling (missing set -e, trap, or || exit)")
    elif is_powershell and ('try' not in script_code.lower() or 'catch' not in script_code.lower()):
        warnings.append("Insufficient error handling (missing try-catch)")

    # Check scope validation
    if not any(kw in script_code.lower() for kw in ['scope', 'validate', 'check', 'allowed', 'authorized']):
        warnings.append("No explicit scope validation detected")

    # Check destructive operations
    destructive_patterns = [r'\brm\b', r'\bdel\b', r'unlink', r'remove', r'delete', r'drop\s+table', r'truncate', r'shutdown', r'reboot', r'os\.system', r'subprocess\.call.*rm']
    for pattern in destructive_patterns:
        if re.search(pattern, script_code, re.IGNORECASE):
            warnings.append(f"Potentially destructive operation found: {pattern}")

    # Syntax validation for Python
    if is_python:
        try:
            ast.parse(script_code)
        except SyntaxError as e:
            warnings.append(f"Python syntax error: {e}")
            return False, warnings

    return len(warnings) == 0, warnings


def add_safety_wrapper(script_code: str, vuln: dict) -> str:
    """Add safety wrapper and metadata header to generated script."""
    host = vuln.get('host_ip', 'unknown')
    port = vuln.get('port', 0)
    cve = vuln.get('cve', 'N/A')
    title = vuln.get('title', 'Unknown')
    plugin_id = vuln.get('raw_plugin_id', 'unknown')

    header = f'''#!/usr/bin/env python3
"""
AUVAP Generated Exploit Script
================================

METADATA:
  Vulnerability: {title}
  CVE: {cve}
  Target: {host}:{port}
  Plugin ID: {plugin_id}
  Generated: {datetime.now().isoformat()}

AUTHORIZATION NOTICE:
  This script is for AUTHORIZED PENETRATION TESTING ONLY.
  Unauthorized access to computer systems is illegal.
  Ensure you have written permission before executing.

SAFETY CONSTRAINTS:
  - Max attempts: 3
  - Timeout: 10 seconds
  - Delay between attempts: 2 seconds
  - Scope validation: Required
  - No destructive actions

USAGE:
  python {cve}_{plugin_id}.py

  Or import and call:
  result = exploit(target_host="{host}", target_port={port})
"""

import time
import sys
from typing import Dict, Any

# Safety configuration
MAX_ATTEMPTS = 3
TIMEOUT_SECONDS = 10
DELAY_BETWEEN_ATTEMPTS = 2
ALLOWED_TARGETS = ["{host}"]  # Modify to include authorized targets

def validate_target(target_host: str) -> bool:
    """Validate target is in authorized scope."""
    if target_host not in ALLOWED_TARGETS:
        print(f"[!] Target {{target_host}} not in authorized scope", file=sys.stderr)
        print(f"[!] Allowed targets: {{ALLOWED_TARGETS}}", file=sys.stderr)
        return False
    return True

'''

    footer = f'''

if __name__ == "__main__":
    """Main execution block with safety checks."""
    target_host = "{host}"
    target_port = {port}

    print("=" * 70)
    print("AUVAP Exploit Script - {title}")
    print("=" * 70)
    print(f"Target: {{target_host}}:{{target_port}}")
    print(f"CVE: {cve}")
    print()

    # Confirm execution
    confirm = input("Execute exploit? (yes/no): ").strip().lower()
    if confirm != "yes":
        print("[*] Execution cancelled by user")
        sys.exit(0)

    # Validate scope
    if not validate_target(target_host):
        sys.exit(1)

    # Execute with safety constraints
    print("[*] Starting exploitation attempt...")
    print(f"[*] Max attempts: {{MAX_ATTEMPTS}}")
    print(f"[*] Timeout: {{TIMEOUT_SECONDS}}s")
    print()

    try:
        result = exploit(target_host=target_host, target_port=target_port, timeout=TIMEOUT_SECONDS)

        print()
        print("=" * 70)
        print("EXPLOITATION RESULT")
        print("=" * 70)
        print(f"Success: {{result.get('success', False)}}")
        print(f"Evidence: {{result.get('evidence', 'N/A')}}")

        if result.get('logs'):
            print()
            print("Execution Logs:")
            for log in result['logs']:
                print(f"  {{log}}")

        print("=" * 70)

    except KeyboardInterrupt:
        print("\\n[!] Exploitation interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\\n[!] Exploitation failed: {{e}}", file=sys.stderr)
        sys.exit(1)
'''

    return header + "\n" + script_code + "\n" + footer


def save_exploit_script(script_code: str, vuln: dict, output_dir: str = "exploits") -> str:
    """Save exploit script to organized directory structure."""
    host = vuln.get('host_ip', 'unknown').replace('.', '_')
    cve = (vuln.get('cve') or 'NO_CVE').replace('-', '_')
    plugin_id = vuln.get('raw_plugin_id', 'unknown')

    # Detect language extension
    first_lines = '\n'.join(script_code.split('\n')[:5]).lower()
    ext_map = {
        'bash': '.sh', 'powershell': '.ps1', 'ruby': '.rb', 'perl': '.pl', 'javascript': '.js'
    }
    extension = ".py"  # Default

    for lang, ext in ext_map.items():
        if f'language: {lang}' in first_lines or script_code.startswith(f'#!/bin/{lang}') or script_code.startswith(f'#!/usr/bin/env {lang}'):
            extension = ext
            break

    # PowerShell specific checks
    if '#Requires' in script_code[:100] or '$ErrorActionPreference' in first_lines or any(kw in first_lines for kw in ['invoke-webrequest', 'test-netconnection', 'param(', 'cmdletbinding']):
        extension = '.ps1'

    # Create directory and save
    host_dir = Path(output_dir) / host
    host_dir.mkdir(parents=True, exist_ok=True)

    script_path = host_dir / f"{cve}_{plugin_id}{extension}"
    script_path.write_text(script_code, encoding='utf-8')

    try:
        script_path.chmod(0o755)
    except:
        pass

    return str(script_path)


def generate_manifest(vuln: dict, script_path: str, safety_warnings: list[str]) -> dict:
    """Generate manifest file for exploit script."""
    return {
        "vulnerability_id": f"{vuln.get('cve', 'NO_CVE')}_{vuln.get('raw_plugin_id', 'unknown')}",
        "cve": vuln.get('cve'),
        "title": vuln.get('title'),
        "target": f"{vuln.get('host_ip', 'unknown')}:{vuln.get('port', 0)}",
        "service": vuln.get('service'),
        "os": vuln.get('os'),
        "script_path": script_path,
        "prerequisites": ["Python 3.8+", "Standard library: socket, requests, ssl", "Authorization from target owner", "Network access to target"],
        "safety_constraints": {"max_attempts": 3, "timeout_seconds": 10, "delay_between_attempts": 2, "scope_validation": True, "destructive_actions": False},
        "safety_warnings": safety_warnings,
        "generated_at": datetime.now().isoformat(),
        "llm_generated": True,
        "requires_review": True
    }


# ============================================================================
# Batch Processing
# ============================================================================

def generate_exploits_from_report(report_file: str, output_dir: str = "exploits",
                                 provider: str = "auto", model: Optional[str] = None) -> dict[str, Any]:
    """
    Generate exploit scripts for all feasible vulnerabilities in report.

    Args:
        report_file: Path to experiment_report.json from Part 3
        output_dir: Directory to save generated scripts
        provider: LLM provider
        model: Optional model name

    Returns:
        Summary dictionary with generation results
    """
    print(f"[*] Loading vulnerability report: {report_file}")
    with open(report_file, 'r', encoding='utf-8') as f:
        report = json.load(f)

    # Create unique output directory
    report_name = Path(report_file).stem
    timestamp = report_name.replace('experiment_report_', '') if 'experiment_report_' in report_name else datetime.now().strftime("%Y%m%d_%H%M%S")
    unique_output_dir = Path(output_dir) / f"exploits_{timestamp}"
    unique_output_dir.mkdir(parents=True, exist_ok=True)

    provider_labels = {"auto": "Auto-detect", "openai": "OpenAI", "gemini": "Google Gemini", "github": "GitHub Models", "local": "Local (Ollama/LM Studio)"}
    print(f"[*] Provider selection: {provider_labels.get(provider, provider)}")
    if model:
        print(f"[*] Requested model: {model}")

    feasible = report.get('feasible_findings_detailed', [])
    if not feasible:
        print("[!] No feasible vulnerabilities found in report")
        return {"total": 0, "generated": 0, "failed": 0, "manifests": []}

    print(f"[*] Found {len(feasible)} feasible vulnerabilities")
    print(f"[*] Output directory: {unique_output_dir}")
    print()

    results = {"total": len(feasible), "generated": 0, "failed": 0, "manifests": [], "failures": []}

    for i, vuln in enumerate(feasible, 1):
        title = vuln.get('title', 'Unknown')[:50]
        print(f"[{i}/{len(feasible)}] Generating exploit for: {title}...")

        try:
            script_code = generate_exploit_script(vuln, provider=provider, model=_normalize_model_alias(provider, model))

            print(f"      Validating script safety...")
            is_safe, warnings = validate_script_safety(script_code)

            if warnings:
                print(f"      ⚠️  Safety warnings: {len(warnings)}")
                for warning in warnings:
                    print(f"        - {warning}")

            # Add wrapper only for Python scripts
            first_line = script_code.split('\n')[0].lower() if script_code else ''
            is_python = '# language: python' in first_line or script_code.startswith('#!/usr/bin/env python') or script_code.startswith('#!/usr/bin/python') or 'def ' in script_code[:200]
            wrapped_script = add_safety_wrapper(script_code, vuln) if is_python else script_code

            script_path = save_exploit_script(wrapped_script, vuln, str(unique_output_dir))
            print(f"      ✅ Saved to: {script_path}")

            manifest = generate_manifest(vuln, script_path, warnings)
            results['manifests'].append(manifest)
            results['generated'] += 1

            if i < len(feasible):
                time.sleep(5)

        except Exception as e:
            print(f"      ❌ Generation failed: {str(e)[:100]}")
            results['failed'] += 1
            results['failures'].append({"vulnerability": title, "error": str(e)})
            continue

        print()

    # Save manifest
    manifest_path = unique_output_dir / "exploits_manifest.json"
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)

    print("=" * 70)
    print("EXPLOIT GENERATION SUMMARY")
    print("=" * 70)
    print(f"Total vulnerabilities: {results['total']}")
    print(f"Successfully generated: {results['generated']}")
    print(f"Failed: {results['failed']}")
    print(f"Output directory: {unique_output_dir}")
    print(f"Manifest saved to: {manifest_path}")
    print("=" * 70)

    return results


def main() -> None:
    """CLI entry point for exploit generator."""
    import argparse

    parser = argparse.ArgumentParser(description="Generate safe exploit scripts from AUVAP vulnerability report")
    parser.add_argument('report', help='Path to experiment_report.json from Part 3')
    parser.add_argument('--output', '-o', default='exploits', help='Output directory for generated scripts (default: exploits/)')
    parser.add_argument('--provider', '-p', choices=['auto', 'github', 'gemini', 'openai', 'local'], default='auto', help='LLM provider (default: auto)')
    parser.add_argument('--model', '-m', help='Model name override (e.g., gpt-4o, gemini-2.0-flash-exp)')

    args = parser.parse_args()

    # Resolve report path
    report_path = args.report
    if not os.path.exists(report_path):
        alt_path = os.path.join('results', os.path.basename(report_path))
        if os.path.exists(alt_path):
            report_path = alt_path
        else:
            print(f"[ERROR] Report file not found: {args.report}", file=sys.stderr)
            print(f"[ERROR] Also checked: {alt_path}", file=sys.stderr)
            sys.exit(1)

    # Check for API keys (not required for local provider)
    has_key = any([os.environ.get('GITHUB_TOKEN'), os.environ.get('GEMINI_API_KEY'), os.environ.get('OPENAI_API_KEY')])
    provider_choice = args.provider
    model_choice = args.model

    # Interactive provider selection if auto
    if provider_choice == 'auto':
        print("Select LLM Provider:")
        print("  1. Auto-detect (default)")
        print("  2. OpenAI")
        print("  3. Google Gemini")
        print("  4. GitHub Models")
        print("  5. Local (Ollama/LM Studio)")
        print()

        choice = input("Choose provider (1-5) [1]: ").strip()
        provider_map = {'1': 'auto', '2': 'openai', '3': 'gemini', '4': 'github', '5': 'local', '': 'auto'}
        provider_choice = provider_map.get(choice, 'auto')

    # Local model selection
    if provider_choice == 'local' and not model_choice:
        print("Select local model:")
        print("  1. deepseek-r1:14b (default)")
        print("  2. qwen3:14b")
        print("  3. Custom model name")
        print()

        local_choice = input("Choose model (1-3) [1]: ").strip()
        if local_choice == '2':
            model_choice = 'qwen3:14b'
        elif local_choice == '3':
            model_choice = input("Enter local model name: ").strip() or 'deepseek-r1:14b'
        else:
            model_choice = 'deepseek-r1:14b'

    # Validate API keys
    if provider_choice == 'openai' and not os.environ.get('OPENAI_API_KEY'):
        print("[ERROR] OPENAI_API_KEY not set", file=sys.stderr)
        sys.exit(1)
    if provider_choice == 'gemini' and not (os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')):
        print("[ERROR] GEMINI_API_KEY not set", file=sys.stderr)
        sys.exit(1)
    if provider_choice == 'github' and not os.environ.get('GITHUB_TOKEN'):
        print("[ERROR] GITHUB_TOKEN not set", file=sys.stderr)
        sys.exit(1)

    # Generate exploits
    try:
        generate_exploits_from_report(report_path, output_dir=args.output, provider=provider_choice, model=model_choice)
    except FileNotFoundError as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] Exploit generation failed: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
