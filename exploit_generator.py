#!/usr/bin/env python3
"""
exploit_generator.py - LLM-Based Exploit Script Generator

This module generates safe, context-aware penetration testing scripts using LLM.
It reads feasible vulnerabilities from the AUVAP pipeline and creates ethical,
validated Python exploit scripts for proof-of-concept testing.

IMPORTANT SECURITY NOTICE:
- All generated scripts are for AUTHORIZED TESTING ONLY
- Scripts include safety constraints and scope validation
- No destructive actions are included
- Proper error handling and logging is enforced
- Scripts must be reviewed before execution

NO UNAUTHORIZED TESTING. ETHICAL USE ONLY.
"""

import ast
import json
import os
import re
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Optional


def _normalize_model_alias(provider: str, model: Optional[str]) -> Optional[str]:
    """Normalize common model aliases to canonical names per provider."""
    if not model:
        return model
    name = model.strip()
    if provider == 'openai':
        # Accept variants like gpt5-thinking, GPT5Thinking, etc.
        lowered = name.lower().replace("_", "-").replace(" ", "-")
        if lowered in {"gpt5-thinking", "gpt-5thinking", "gpt5thinking"}:
            return "gpt-5-thinking"
    return model


def _generate_with_openai(prompt: str, api_key: str, model: str,
                           base_url: str = "https://api.openai.com/v1") -> str:
    """Call OpenAI-compatible chat completions endpoint and return text."""
    try:
        from openai import OpenAI
    except ImportError as exc:
        raise ImportError("openai package not installed. Run: pip install openai") from exc

    client = OpenAI(api_key=api_key, base_url=base_url)
    response = client.chat.completions.create(
        model=model,
        messages=[
            {
                "role": "system",
                "content": (
                    "You are a senior penetration tester generating safe proof-of-concept scripts. "
                    "Respond with Python code only."
                ),
            },
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        max_tokens=1500,
    )

    message_content = response.choices[0].message.content or ""
    return message_content.strip()


def _generate_with_openai_responses(prompt: str, api_key: str, model: str,
                                    base_url: str = "https://api.openai.com/v1",
                                    reasoning_effort: str = "high",
                                    verbosity: str = "low") -> str:
    """Call OpenAI Responses API for reasoning models and return text output."""
    try:
        from openai import OpenAI
    except ImportError as exc:
        raise ImportError("openai package not installed. Run: pip install openai") from exc

    client = OpenAI(api_key=api_key, base_url=base_url)
    resp = client.responses.create(
        model=model,
        input=prompt,
        reasoning={"effort": reasoning_effort},  # type: ignore[arg-type]
        text={"verbosity": verbosity},  # type: ignore[arg-type]
    )

    text = getattr(resp, "output_text", None)
    if isinstance(text, str) and text.strip():
        return text.strip()

    # Fallback: try to concatenate text parts
    try:
        parts = []
        output = getattr(resp, "output", None)
        if output:
            for item in output:
                content = getattr(item, "content", None)
                if content:
                    for c in content:
                        if getattr(c, "type", "") in ("output_text", "text") and getattr(c, "text", ""):
                            parts.append(c.text)
        if parts:
            return "\n".join(parts).strip()
    except Exception:
        pass
    return ""


def _is_openai_reasoning_model(model: Optional[str]) -> bool:
    if not model:
        return False
    m = model.lower()
    return m.startswith("gpt-5") or m.startswith("o4") or m.startswith("o3") or ("reason" in m)


def _generate_with_gemini(prompt: str, api_key: str,
                           model: str = "gemini-2.0-flash-exp") -> str:
    """Call Google Gemini content generation and return text."""
    try:
        from google import genai  # type: ignore[import]
        from google.genai import types  # type: ignore[import]
    except ImportError as exc:
        raise ImportError("google-genai package not installed. Run: pip install google-genai") from exc

    client = genai.Client(api_key=api_key)
    response = client.models.generate_content(
        model=model,
        contents=prompt,
        config=types.GenerateContentConfig(
            temperature=0.2,
            max_output_tokens=1600,
        ),
    )
    return response.text.strip()


def choose_language_for_vuln(vuln: dict) -> str:
    """
    Determine the best scripting language for a vulnerability.
    
    Args:
        vuln: Vulnerability dictionary
        
    Returns:
        Language choice: 'bash', 'powershell', 'python', or 'ruby'
    """
    service = (vuln.get('service') or '').lower()
    title = (vuln.get('title') or '').lower()
    os_type = (vuln.get('os') or '').lower()
    cve = (vuln.get('cve') or '').lower()
    port = vuln.get('port', 0)
    
    # Windows-specific vulnerabilities → PowerShell
    if 'windows' in os_type or 'microsoft' in title:
        return 'powershell'
    
    # Service-based detection for Windows services
    if service in ['smb', 'microsoft-ds', 'netbios-ssn', 'msrpc', 'rdp', 'ms-wbt-server', 'winrm']:
        return 'powershell'
    
    # IIS and Windows web services
    if 'iis' in service or 'iis' in title:
        return 'powershell'
    
    # Known CVEs with established exploit languages
    cve_language_map = {
        'cve-2017-0144': 'powershell',  # MS17-010 (EternalBlue)
        'cve-2017-0145': 'powershell',  # MS17-010 variant
        'cve-2014-6271': 'bash',         # Shellshock
        'cve-2014-7169': 'bash',         # Shellshock variant
        'cve-2017-7269': 'powershell',  # IIS WebDAV
        'cve-2019-0708': 'powershell',  # BlueKeep (RDP)
        'cve-2020-0796': 'powershell',  # SMBGhost
    }
    
    if cve in cve_language_map:
        return cve_language_map[cve]
    
    # Unix/Linux services → Bash
    linux_services = ['ssh', 'ftp', 'telnet', 'smtp', 'pop3', 'imap', 'nntp', 'finger']
    if service in linux_services:
        return 'bash'
    
    # Web services and APIs → Python (most versatile)
    web_services = ['http', 'https', 'apache', 'tomcat', 'nginx', 'jenkins', 'wordpress', 'joomla']
    if service in web_services or any(ws in title for ws in ['http', 'web', 'apache', 'tomcat', 'nginx']):
        return 'python'
    
    # Databases → Python (best library support)
    if any(db in service for db in ['postgresql', 'mysql', 'mariadb', 'mongodb', 'redis', 'oracle', 'mssql']):
        return 'python'
    
    # Keyword-based detection
    if 'shellshock' in title or 'bash' in title:
        return 'bash'
    
    if 'command injection' in title and 'windows' not in os_type:
        return 'bash'
    
    if 'buffer overflow' in title or 'memory corruption' in title:
        return 'python'  # Python for complex exploits
    
    # Port-based heuristics
    if port in [445, 139, 135, 3389]:  # SMB, NetBIOS, RPC, RDP
        return 'powershell'
    
    if port in [22, 21, 23, 25]:  # SSH, FTP, Telnet, SMTP
        return 'bash'
    
    # Default to Python for complex/unknown exploits
    return 'python'


def build_exploit_prompt(vuln: dict, target_language: str) -> str:
    """
    Build LLM prompt for generating safe exploit script in specified language.
    
    Args:
        vuln: Vulnerability dictionary from feasibility filter
        target_language: Target language (bash, powershell, python, ruby)
        
    Returns:
        Formatted prompt string for LLM
    """
    host = vuln.get('host_ip', 'unknown')
    port = vuln.get('port', 0)
    service = vuln.get('service', 'unknown')
    os_info = vuln.get('os', 'Unknown')
    title = vuln.get('title', 'Unknown Vulnerability')
    cve = vuln.get('cve', 'N/A')
    description = vuln.get('description', 'No description available')
    exploit_notes = vuln.get('exploit_notes', '')
    vuln_component = vuln.get('vuln_component', 'unknown')
    
    # Language-specific instructions
    language_instructions = {
        'bash': """Generate a Bash script (.sh) using standard Unix tools (curl, nc, ssh, openssl, etc.).
- Start with #!/bin/bash
- Use bash error handling (set -e, trap, || exit)
- Use 'timeout' command for timeouts
- Output results to stdout""",
        
        'powershell': """Generate a PowerShell script (.ps1) using native cmdlets.
- Use Invoke-WebRequest, Invoke-Command, Test-NetConnection, etc.
- Use try-catch-finally for error handling
- Set $ErrorActionPreference = 'Stop'
- Use -TimeoutSec for timeouts
- Return PSCustomObject with Success, Evidence, Logs properties""",
        
        'python': """Generate a Python 3 script (.py) using standard libraries.
- Use requests, socket, paramiko, ssl, http.client when needed
- Use try-except-finally for error handling
- Include function: def exploit(target_host: str, target_port: int, timeout: int = 10) -> dict
- Return dict with keys: success (bool), evidence (str), logs (list)""",
        
        'ruby': """Generate a Ruby script (.rb) using standard libraries or Metasploit.
- Use Net::HTTP, Socket, OpenSSL when needed
- Use begin-rescue-ensure for error handling
- Include method: def exploit(target_host, target_port, timeout = 10)
- Return hash with keys: success, evidence, logs"""
    }
    
    lang_instruction = language_instructions.get(target_language, language_instructions['python'])
    lang_name = target_language.capitalize()
    
    prompt = f"""Generate a safe, ethical penetration testing script in {lang_name.upper()} for this vulnerability:

Target: {host}:{port}
Service: {service}
Operating System: {os_info}
Vulnerability: {title}
CVE: {cve}
Vulnerable Component: {vuln_component}

Description: {description}

Exploit Guidance: {exploit_notes}

LANGUAGE REQUIREMENTS:
{lang_instruction}

SAFETY REQUIREMENTS:
- Include timeout: 10 seconds maximum
- Max attempts: 3
- Delay between attempts: 2 seconds
- Validate target is in scope before ANY network activity
- Add detailed logging/output for each step
- DO NOT include destructive actions (file deletion, service shutdown, etc.)
- Focus on proof-of-concept validation ONLY (verify vulnerability exists)

OUTPUT REQUIREMENTS:
At the TOP of your response, specify the language on the first line as a comment:
# Language: {lang_name}

Then provide ONLY the complete code with no markdown formatting, no explanations, no preamble.
Start directly with language declaration (# Language: {lang_name}) followed by code."""

    return prompt


def generate_exploit_script(vuln: dict, provider: str = "auto", model: Optional[str] = None) -> str:
    """
    Generate exploit script using LLM.
    
    Args:
        vuln: Vulnerability dictionary
        provider: LLM provider ("auto", "github", "gemini", "openai", "local")
        model: Optional model name override
        
    Returns:
        Generated exploit script code in appropriate language
        
    Raises:
        RuntimeError: If LLM generation fails
    """
    # Choose language based on vulnerability characteristics
    target_language = choose_language_for_vuln(vuln)
    print(f"      Chosen language: {target_language}", file=sys.stderr)
    
    # Build prompt with target language
    prompt = build_exploit_prompt(vuln, target_language)
    
    # Detect provider and get API key
    actual_provider = provider
    api_key = None

    if provider == "auto":
        if os.environ.get('GITHUB_TOKEN'):
            actual_provider = "github"
            api_key = os.environ['GITHUB_TOKEN']
        elif os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY'):
            actual_provider = "gemini"
            api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')
        elif os.environ.get('OPENAI_API_KEY'):
            actual_provider = "openai"
            api_key = os.environ['OPENAI_API_KEY']
        else:
            # Fall back to local provider if configured
            actual_provider = "local"
            api_key = None
    else:
        if provider == "github":
            api_key = os.environ.get('GITHUB_TOKEN')
            if not api_key:
                raise RuntimeError("GITHUB_TOKEN not set for GitHub provider")
        elif provider == "gemini":
            api_key = os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')
            if not api_key:
                raise RuntimeError("GEMINI_API_KEY not set for Gemini provider")
        elif provider == "openai":
            api_key = os.environ.get('OPENAI_API_KEY')
            if not api_key:
                raise RuntimeError("OPENAI_API_KEY not set for OpenAI provider")
        elif provider == "local":
            actual_provider = "local"
            api_key = None
    
    # Generate script using appropriate provider
    print(f"      Generating exploit using {actual_provider}...", file=sys.stderr)
    
    script: Any
    if actual_provider == "github":
        assert api_key, "GITHUB_TOKEN not available"
        model_candidates = [model] if model else ["gpt-4o-mini", "gpt-4o"]
        last_error: Optional[Exception] = None
        for candidate in model_candidates:
            try:
                script = _generate_with_openai(
                    prompt,
                    api_key,
                    candidate,
                    base_url="https://models.inference.ai.azure.com",
                )
                break
            except Exception as err:  # noqa: BLE001
                last_error = err
                error_text = str(err)
                if "does not exist" in error_text or "404" in error_text:
                    print(
                        f"      ⚠️  Model {candidate} unavailable, trying fallback...",
                        file=sys.stderr,
                    )
                    continue
                raise
        else:
            assert last_error is not None
            raise last_error
    elif actual_provider == "gemini":
        assert api_key, "GEMINI_API_KEY not available"
        gemini_models = [model] if model else ["gemini-2.0-flash-exp", "gemini-1.5-flash"]
        last_error = None
        for candidate in gemini_models:
            try:
                script = _generate_with_gemini(prompt, api_key, model=candidate)
                break
            except Exception as err:  # noqa: BLE001
                last_error = err
                error_text = str(err)
                if "404" in error_text or "not found" in error_text.lower():
                    print(
                        f"      ⚠️  Model {candidate} unavailable, trying fallback...",
                        file=sys.stderr,
                    )
                    continue
                raise
        else:
            assert last_error is not None
            raise last_error
    elif actual_provider == "openai":
        assert api_key, "OPENAI_API_KEY not available"
        openai_models = [model] if model else ["gpt-5", "gpt-4o-mini", "gpt-5-nano"]
        last_error = None
        for candidate in openai_models:
            try:
                print(f"      -> Attempting model: {candidate}", file=sys.stderr)
                # Use Responses API for reasoning models like gpt-5
                if _is_openai_reasoning_model(candidate):
                    script = _generate_with_openai_responses(prompt, api_key, candidate)
                else:
                    script = _generate_with_openai(prompt, api_key, candidate)
                break
            except Exception as err:  # noqa: BLE001
                last_error = err
                error_text = str(err)
                if (
                    "does not exist" in error_text
                    or "404" in error_text
                    or "not found" in error_text.lower()
                    or "429" in error_text
                    or "quota" in error_text.lower()
                    or "rate limit" in error_text.lower()
                    or "exceeded your current quota" in error_text.lower()
                ):
                    print(
                        f"      ⚠️  Model {candidate} unavailable, trying fallback...",
                        file=sys.stderr,
                    )
                    # Small backoff if it's a rate/quota issue
                    if "429" in error_text or "quota" in error_text.lower() or "rate limit" in error_text.lower():
                        time.sleep(3)
                    continue
                raise
        else:
            assert last_error is not None
            raise last_error
    elif actual_provider == "local":
        # OpenAI-compatible local server (Ollama/LM Studio)
        base_url = os.environ.get('LOCAL_OPENAI_BASE_URL') or 'http://localhost:11434/v1'
        model_candidates = [model] if model else ["deepseek-r1:14b", "qwen3:14b"]
        last_error = None
        for candidate in model_candidates:
            try:
                print(f"      -> Attempting local model: {candidate}", file=sys.stderr)
                script = _generate_with_openai(
                    prompt,
                    api_key or "local",
                    candidate,
                    base_url=base_url,
                )
                break
            except Exception as err:  # noqa: BLE001
                last_error = err
                print(f"      ⚠️  Local model {candidate} failed: {str(err)[:120]}", file=sys.stderr)
                print("         Hint: Ensure Ollama or LM Studio is running and LOCAL_OPENAI_BASE_URL is set if not default.", file=sys.stderr)
                continue
        else:
            assert last_error is not None
            raise last_error
    else:
        raise ValueError(f"Unknown provider: {actual_provider}")
    
    # Extract code from response (LLM might return JSON or direct code)
    if isinstance(script, dict):
        # If response is dict, look for code-like keys
        code = script.get('code') or script.get('exploit_code') or script.get('script') or str(script)
    else:
        code = str(script)
    
    # Clean up code (remove markdown, extra whitespace)
    code = code.strip()
    
    # Remove markdown code blocks if present
    if code.startswith('```'):
        lines = code.split('\n')
        # Find first and last ``` markers
        start_idx = 0
        end_idx = len(lines)
        
        for i, line in enumerate(lines):
            if line.startswith('```'):
                if start_idx == 0:
                    start_idx = i + 1
                else:
                    end_idx = i
                    break
        
        code = '\n'.join(lines[start_idx:end_idx])
    
    return code.strip()


def validate_script_safety(script_code: str) -> tuple[bool, list[str]]:
    """
    Validate generated script for safety issues.
    
    Checks for:
    - No hardcoded credentials
    - Proper scope checking
    - Timeout mechanisms
    - Error handling
    - No destructive operations
    
    Args:
        script_code: Script code to validate (any language)
        
    Returns:
        Tuple of (is_safe: bool, warnings: list[str])
    """
    warnings = []
    
    # Detect script language
    first_line = script_code.split('\n')[0].lower() if script_code else ''
    is_python = (
        '# language: python' in first_line or 
        script_code.startswith('#!/usr/bin/env python') or
        script_code.startswith('#!/usr/bin/python')
    )
    is_bash = '# language: bash' in first_line or script_code.startswith('#!/bin/bash')
    is_powershell = '# language: powershell' in first_line or script_code.startswith('#Requires')
    
    # Check for hardcoded credentials (basic patterns)
    credential_patterns = [
        r'password\s*=\s*["\'][^"\']+["\']',
        r'passwd\s*=\s*["\'][^"\']+["\']',
        r'api_key\s*=\s*["\'][^"\']+["\']',
        r'secret\s*=\s*["\'][^"\']+["\']',
        r'token\s*=\s*["\'][a-zA-Z0-9]{20,}["\']'
    ]
    
    for pattern in credential_patterns:
        if re.search(pattern, script_code, re.IGNORECASE):
            warnings.append(f"Potential hardcoded credential found: {pattern}")
    
    # Check for timeout mechanisms
    if 'timeout' not in script_code.lower():
        warnings.append("No timeout mechanism detected")
    
    # Check for error handling (language-specific)
    if is_python:
        if 'try:' not in script_code or 'except' not in script_code:
            warnings.append("Insufficient error handling (missing try-except)")
    elif is_bash:
        if 'set -e' not in script_code and 'trap' not in script_code and '|| exit' not in script_code:
            warnings.append("Insufficient error handling (missing set -e, trap, or || exit)")
    elif is_powershell:
        if 'try' not in script_code.lower() or 'catch' not in script_code.lower():
            warnings.append("Insufficient error handling (missing try-catch)")
    
    # Check for scope validation
    scope_keywords = ['scope', 'validate', 'check', 'allowed', 'authorized']
    has_scope_check = any(keyword in script_code.lower() for keyword in scope_keywords)
    if not has_scope_check:
        warnings.append("No explicit scope validation detected")
    
    # Check for destructive operations
    destructive_patterns = [
        r'\brm\b', r'\bdel\b', r'unlink', r'remove', r'delete',
        r'drop\s+table', r'truncate', r'shutdown', r'reboot',
        r'os\.system', r'subprocess\.call.*rm'
    ]
    
    for pattern in destructive_patterns:
        if re.search(pattern, script_code, re.IGNORECASE):
            warnings.append(f"Potentially destructive operation found: {pattern}")
    
    # Syntax validation only for Python scripts
    if is_python:
        try:
            ast.parse(script_code)
        except SyntaxError as e:
            warnings.append(f"Python syntax error: {e}")
            return False, warnings
    # Skip syntax validation for Bash/PowerShell (would need language-specific parsers)
    
    # Script is considered safe if no critical warnings
    is_safe = len(warnings) == 0
    
    return is_safe, warnings


def add_safety_wrapper(script_code: str, vuln: dict) -> str:
    """
    Add safety wrapper and metadata header to generated script.
    
    Args:
        script_code: Original generated script
        vuln: Vulnerability metadata
        
    Returns:
        Enhanced script with safety wrapper
    """
    host = vuln.get('host_ip', 'unknown')
    port = vuln.get('port', 0)
    cve = vuln.get('cve', 'N/A')
    title = vuln.get('title', 'Unknown')
    plugin_id = vuln.get('raw_plugin_id', 'unknown')
    
    header = f'''#!/usr/bin/env python3
"""
AUVAP Generated Exploit Script
================================

METADATA:
  Vulnerability: {title}
  CVE: {cve}
  Target: {host}:{port}
  Plugin ID: {plugin_id}
  Generated: {datetime.now().isoformat()}
  
AUTHORIZATION NOTICE:
  This script is for AUTHORIZED PENETRATION TESTING ONLY.
  Unauthorized access to computer systems is illegal.
  Ensure you have written permission before executing.
  
SAFETY CONSTRAINTS:
  - Max attempts: 3
  - Timeout: 10 seconds
  - Delay between attempts: 2 seconds
  - Scope validation: Required
  - No destructive actions
  
USAGE:
  python {cve}_{plugin_id}.py
  
  Or import and call:
  result = exploit(target_host="{host}", target_port={port})
"""

import time
import sys
from typing import Dict, Any

# Safety configuration
MAX_ATTEMPTS = 3
TIMEOUT_SECONDS = 10
DELAY_BETWEEN_ATTEMPTS = 2
ALLOWED_TARGETS = ["{host}"]  # Modify to include authorized targets

def validate_target(target_host: str) -> bool:
    """Validate target is in authorized scope."""
    if target_host not in ALLOWED_TARGETS:
        print(f"[!] Target {{target_host}} not in authorized scope", file=sys.stderr)
        print(f"[!] Allowed targets: {{ALLOWED_TARGETS}}", file=sys.stderr)
        return False
    return True

'''

    footer = f'''

if __name__ == "__main__":
    """
    Main execution block with safety checks.
    """
    target_host = "{host}"
    target_port = {port}
    
    print("=" * 70)
    print("AUVAP Exploit Script - {title}")
    print("=" * 70)
    print(f"Target: {{target_host}}:{{target_port}}")
    print(f"CVE: {cve}")
    print()
    
    # Confirm execution
    confirm = input("Execute exploit? (yes/no): ").strip().lower()
    if confirm != "yes":
        print("[*] Execution cancelled by user")
        sys.exit(0)
    
    # Validate scope
    if not validate_target(target_host):
        sys.exit(1)
    
    # Execute with safety constraints
    print("[*] Starting exploitation attempt...")
    print(f"[*] Max attempts: {{MAX_ATTEMPTS}}")
    print(f"[*] Timeout: {{TIMEOUT_SECONDS}}s")
    print()
    
    try:
        result = exploit(
            target_host=target_host,
            target_port=target_port,
            timeout=TIMEOUT_SECONDS
        )
        
        print()
        print("=" * 70)
        print("EXPLOITATION RESULT")
        print("=" * 70)
        print(f"Success: {{result.get('success', False)}}")
        print(f"Evidence: {{result.get('evidence', 'N/A')}}")
        
        if result.get('logs'):
            print()
            print("Execution Logs:")
            for log in result['logs']:
                print(f"  {{log}}")
        
        print("=" * 70)
        
    except KeyboardInterrupt:
        print("\\n[!] Exploitation interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\\n[!] Exploitation failed: {{e}}", file=sys.stderr)
        sys.exit(1)
'''

    # Combine header + generated code + footer
    full_script = header + "\n" + script_code + "\n" + footer
    
    return full_script


def save_exploit_script(script_code: str, vuln: dict, output_dir: str = "exploits") -> str:
    """
    Save exploit script to organized directory structure.
    
    Args:
        script_code: Complete script code with wrapper
        vuln: Vulnerability metadata
        output_dir: Base output directory
        
    Returns:
        Path to saved script file
    """
    host = vuln.get('host_ip', 'unknown').replace('.', '_')
    cve = (vuln.get('cve') or 'NO_CVE').replace('-', '_')
    plugin_id = vuln.get('raw_plugin_id', 'unknown')
    
    # Detect language from script content (multiple detection methods)
    extension = ".py"  # Default
    first_lines = '\n'.join(script_code.split('\n')[:5]).lower()  # Check first 5 lines
    
    # Method 1: Check for explicit language declaration
    if '# language: bash' in first_lines or 'language: bash' in first_lines:
        extension = ".sh"
    elif '# language: powershell' in first_lines or 'language: powershell' in first_lines:
        extension = ".ps1"
    elif '# language: ruby' in first_lines or 'language: ruby' in first_lines:
        extension = ".rb"
    elif '# language: perl' in first_lines or 'language: perl' in first_lines:
        extension = ".pl"
    elif '# language: javascript' in first_lines or 'language: javascript' in first_lines:
        extension = ".js"
    # Method 2: Check for shebang
    elif script_code.startswith('#!/bin/bash') or script_code.startswith('#!/usr/bin/env bash'):
        extension = ".sh"
    elif script_code.startswith('#Requires') or '$ErrorActionPreference' in first_lines:
        extension = ".ps1"
    elif script_code.startswith('#!/usr/bin/env ruby'):
        extension = ".rb"
    elif script_code.startswith('#!/usr/bin/perl'):
        extension = ".pl"
    elif script_code.startswith('#!/usr/bin/env node'):
        extension = ".js"
    # Method 3: Check for language-specific keywords/syntax
    elif any(keyword in first_lines for keyword in ['invoke-webrequest', 'test-netconnection', 'param(', 'cmdletbinding']):
        extension = ".ps1"
    elif any(keyword in script_code[:500] for keyword in ['curl ', 'nc ', 'wget ', 'echo $', '${', 'function ']):
        # Only if it doesn't look like Python
        if 'def ' not in script_code[:200] and 'import ' not in script_code[:200]:
            extension = ".sh"
    
    # Create directory structure: exploits/{host}/
    host_dir = Path(output_dir) / host
    host_dir.mkdir(parents=True, exist_ok=True)
    
    # Generate filename with detected extension
    filename = f"{cve}_{plugin_id}{extension}"
    script_path = host_dir / filename
    
    # Write script
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(script_code)
    
    # Make executable (Unix-like systems)
    try:
        script_path.chmod(0o755)
    except:
        pass  # Windows doesn't support chmod
    
    return str(script_path)


def generate_manifest(vuln: dict, script_path: str, safety_warnings: list[str]) -> dict:
    """
    Generate manifest file for exploit script.
    
    Args:
        vuln: Vulnerability metadata
        script_path: Path to generated script
        safety_warnings: List of safety validation warnings
        
    Returns:
        Manifest dictionary
    """
    host = vuln.get('host_ip', 'unknown')
    port = vuln.get('port', 0)
    
    manifest = {
        "vulnerability_id": f"{vuln.get('cve', 'NO_CVE')}_{vuln.get('raw_plugin_id', 'unknown')}",
        "cve": vuln.get('cve'),
        "title": vuln.get('title'),
        "target": f"{host}:{port}",
        "service": vuln.get('service'),
        "os": vuln.get('os'),
        "script_path": script_path,
        "prerequisites": [
            "Python 3.8+",
            "Standard library: socket, requests, ssl",
            "Authorization from target owner",
            "Network access to target"
        ],
        "safety_constraints": {
            "max_attempts": 3,
            "timeout_seconds": 10,
            "delay_between_attempts": 2,
            "scope_validation": True,
            "destructive_actions": False
        },
        "safety_warnings": safety_warnings,
        "generated_at": datetime.now().isoformat(),
        "llm_generated": True,
        "requires_review": True
    }
    
    return manifest


def generate_exploits_from_report(
    report_file: str,
    output_dir: str = "exploits",
    provider: str = "auto",
    model: Optional[str] = None
) -> dict[str, Any]:
    """
    Generate exploit scripts for all feasible vulnerabilities in report.
    
    Args:
        report_file: Path to experiment_report.json from Part 3
        output_dir: Directory to save generated scripts
        provider: LLM provider
        model: Optional model name
        
    Returns:
        Summary dictionary with generation results
    """
    # Load report
    print(f"[*] Loading vulnerability report: {report_file}")
    with open(report_file, 'r', encoding='utf-8') as f:
        report = json.load(f)
    
    # Create unique output directory based on report filename and timestamp
    report_name = Path(report_file).stem  # Get filename without extension
    # Extract timestamp from report name if present (format: experiment_report_YYYYMMDD_HHMMSS)
    if 'experiment_report_' in report_name:
        timestamp = report_name.replace('experiment_report_', '')
    else:
        # Generate new timestamp if not found in filename
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Create unique output directory: exploits/exploits_YYYYMMDD_HHMMSS/
    unique_output_dir = Path(output_dir) / f"exploits_{timestamp}"
    unique_output_dir.mkdir(parents=True, exist_ok=True)
    
    provider_labels = {
        "auto": "Auto-detect",
        "openai": "OpenAI",
        "gemini": "Google Gemini",
        "github": "GitHub Models",
        "local": "Local (Ollama/LM Studio)"
    }

    print(f"[*] Provider selection: {provider_labels.get(provider, provider)}")
    if model:
        print(f"[*] Requested model: {model}")

    feasible = report.get('feasible_findings_detailed', [])
    
    if not feasible:
        print("[!] No feasible vulnerabilities found in report")
        return {"total": 0, "generated": 0, "failed": 0, "manifests": []}
    
    print(f"[*] Found {len(feasible)} feasible vulnerabilities")
    print(f"[*] Output directory: {unique_output_dir}")
    print()
    
    results = {
        "total": len(feasible),
        "generated": 0,
        "failed": 0,
        "manifests": [],
        "failures": []
    }
    
    for i, vuln in enumerate(feasible, 1):
        title = vuln.get('title', 'Unknown')[:50]
        print(f"[{i}/{len(feasible)}] Generating exploit for: {title}...")
        
        try:
            # Generate script using LLM
            script_code = generate_exploit_script(vuln, provider=provider, model=_normalize_model_alias(provider, model))
            
            # Validate safety
            print(f"      Validating script safety...")
            is_safe, warnings = validate_script_safety(script_code)
            
            if warnings:
                print(f"      ⚠️  Safety warnings: {len(warnings)}")
                for warning in warnings:
                    print(f"        - {warning}")
            
            # Detect script language to decide if we need Python wrapper
            first_line = script_code.split('\n')[0].lower() if script_code else ''
            is_python = (
                '# language: python' in first_line or 
                script_code.startswith('#!/usr/bin/env python') or
                script_code.startswith('#!/usr/bin/python') or
                'def ' in script_code[:200]  # Likely Python if has 'def' early
            )
            
            # Only add Python safety wrapper for Python scripts
            if is_python:
                wrapped_script = add_safety_wrapper(script_code, vuln)
            else:
                # For non-Python scripts (Bash/PowerShell), save as-is
                wrapped_script = script_code
            
            # Save script (use unique_output_dir instead of output_dir)
            script_path = save_exploit_script(wrapped_script, vuln, str(unique_output_dir))
            print(f"      ✅ Saved to: {script_path}")
            
            # Generate manifest
            manifest = generate_manifest(vuln, script_path, warnings)
            results['manifests'].append(manifest)
            
            results['generated'] += 1
            
            # Rate limiting
            if i < len(feasible):
                time.sleep(5)  # 5s delay between generations
            
        except Exception as e:
            print(f"      ❌ Generation failed: {str(e)[:100]}")
            results['failed'] += 1
            results['failures'].append({
                "vulnerability": title,
                "error": str(e)
            })
            continue
        
        print()
    
    # Save combined manifest to unique directory
    manifest_path = unique_output_dir / "exploits_manifest.json"
    with open(manifest_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2)
    
    print("=" * 70)
    print("EXPLOIT GENERATION SUMMARY")
    print("=" * 70)
    print(f"Total vulnerabilities: {results['total']}")
    print(f"Successfully generated: {results['generated']}")
    print(f"Failed: {results['failed']}")
    print(f"Output directory: {unique_output_dir}")
    print(f"Manifest saved to: {manifest_path}")
    print("=" * 70)
    
    return results


def main() -> None:
    """CLI entry point for exploit generator."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Generate safe exploit scripts from AUVAP vulnerability report"
    )
    parser.add_argument(
        'report',
        help='Path to experiment_report.json from Part 3'
    )
    parser.add_argument(
        '--output', '-o',
        default='exploits',
        help='Output directory for generated scripts (default: exploits/)'
    )
    parser.add_argument(
        '--provider', '-p',
        choices=['auto', 'github', 'gemini', 'openai', 'local'],
        default='auto',
        help='LLM provider (default: auto)'
    )
    parser.add_argument(
        '--model', '-m',
        help='Model name override (e.g., gpt-4o, gemini-2.0-flash-exp)'
    )
    
    args = parser.parse_args()
    
    # Resolve report path - check if file exists, try results/ folder if not
    report_path = args.report
    if not os.path.exists(report_path):
        # Try results/ folder
        alt_path = os.path.join('results', os.path.basename(report_path))
        if os.path.exists(alt_path):
            report_path = alt_path
        else:
            print(f"[ERROR] Report file not found: {args.report}", file=sys.stderr)
            print(f"[ERROR] Also checked: {alt_path}", file=sys.stderr)
            sys.exit(1)
    
    # Check for API keys (not required for local provider)
    has_key = any([
        os.environ.get('GITHUB_TOKEN'),
        os.environ.get('GEMINI_API_KEY'),
        os.environ.get('OPENAI_API_KEY')
    ])

    provider_choice = args.provider
    model_choice = args.model

    if provider_choice == 'auto':
        print("Select LLM Provider:")
        print("  1. Auto-detect (default)")
        print("  2. OpenAI")
        print("  3. Google Gemini")
        print("  4. GitHub Models")
        print("  5. Local (Ollama/LM Studio)")
        print()

        choice = input("Choose provider (1-5) [1]: ").strip()
        provider_map = {
            '1': 'auto',
            '2': 'openai',
            '3': 'gemini',
            '4': 'github',
            '5': 'local',
            '': 'auto'
        }
        provider_choice = provider_map.get(choice, 'auto')

    # If local provider selected and no model specified, prompt for local model
    if provider_choice == 'local' and not model_choice:
        print("Select local model:")
        print("  1. deepseek-r1:14b (default)")
        print("  2. qwen3:14b")
        print("  3. Custom model name")
        print()

        local_choice = input("Choose model (1-3) [1]: ").strip()
        if local_choice == '2':
            model_choice = 'qwen3:14b'
        elif local_choice == '3':
            custom_name = input("Enter local model name: ").strip()
            model_choice = custom_name or 'deepseek-r1:14b'
        else:
            model_choice = 'deepseek-r1:14b'

    # Validate API key availability for explicit provider choices
    if provider_choice == 'openai' and not os.environ.get('OPENAI_API_KEY'):
        print("[ERROR] OPENAI_API_KEY not set", file=sys.stderr)
        sys.exit(1)
    if provider_choice == 'gemini' and not (os.environ.get('GEMINI_API_KEY') or os.environ.get('GOOGLE_API_KEY')):
        print("[ERROR] GEMINI_API_KEY not set", file=sys.stderr)
        sys.exit(1)
    if provider_choice == 'github' and not os.environ.get('GITHUB_TOKEN'):
        print("[ERROR] GITHUB_TOKEN not set", file=sys.stderr)
        sys.exit(1)
    # Allow 'local' without keys
    
    # Generate exploits
    try:
        generate_exploits_from_report(
            report_path,
            output_dir=args.output,
            provider=provider_choice,
            model=model_choice
        )
    except FileNotFoundError as e:
        print(f"[ERROR] {e}", file=sys.stderr)
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] Exploit generation failed: {e}", file=sys.stderr)
        sys.exit(1)


if __name__ == "__main__":
    main()
