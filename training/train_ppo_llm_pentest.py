"""
Unified PPO Training for Pentest with LLM Script Generation

This script creates a complete training environment that:
1. Uses LLM to generate exploit scripts dynamically
2. Applies sequential masking (one task at a time)
3. Trains PPO agent to learn optimal exploitation strategies
4. Stores successful attempts in persistent memory

Usage:
    python training/train_ppo_llm_pentest.py \\
        --nessus-scan data/sample_scan.xml \\
        --timesteps 100000 \\
        --use-llm-generation \\
        --real-execution
"""

import os
import sys
from pathlib import Path
from datetime import datetime
import argparse
from typing import Optional, Dict, Tuple

# Add parent to path
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import numpy as np
import gymnasium as gym
from gymnasium import spaces
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from stable_baselines3.common.vec_env import DummyVecEnv

# AUVAP imports
from parser import parse_nessus_xml, VAFinding
from priority_masking import PriorityMasker
from task_manager import TaskManager
from execution.sandbox_executor import SandboxExecutor, ExecutionResult
from execution.persistent_memory import PersistentMemory
from execution.llm_drl_bridge import LLMDRLBridge
from exploit_generator import ExploitGenerator


class LLMPentestEnv(gym.Env):
    """
    Pentest environment that uses LLM to generate exploit scripts
    and sequential masking to expose one task at a time.

    Environment Flow:
    1. PPO agent selects an action (exploit to run)
    2. Environment checks if action is valid (sequential mask)
    3. LLM generates exploit script for the selected task
    4. Script executes in sandbox with safety constraints
    5. PPO receives reward based on success/failure
    6. Successful scripts stored in persistent memory
    7. Next task exposed sequentially
    """

    def __init__(self,
                 priority_masker: PriorityMasker,
                 llm_bridge: Optional[LLMDRLBridge] = None,
                 use_llm_generation: bool = True,
                 use_real_execution: bool = False,
                 max_steps: int = 100,
                 reward_success: float = 100.0,
                 reward_failure: float = -10.0,
                 reward_step: float = -1.0,
                 verbose: bool = True):
        """
        Initialize LLM-powered pentest environment.

        Args:
            priority_masker: PriorityMasker with loaded tasks
            llm_bridge: LLM-DRL bridge for script generation
            use_llm_generation: Generate scripts via LLM (vs pre-generated)
            use_real_execution: Execute actual scripts (vs simulation)
            max_steps: Max steps per episode
            reward_success: Reward for successful exploit
            reward_failure: Penalty for failed exploit
            reward_step: Step penalty
            verbose: Print detailed logs
        """
        super().__init__()

        self.masker = priority_masker
        self.llm_bridge = llm_bridge
        self.use_llm = use_llm_generation
        self.use_real_exec = use_real_execution
        self.max_steps = max_steps
        self.reward_success = reward_success
        self.reward_failure = reward_failure
        self.reward_step = reward_step
        self.verbose = verbose

        # Track findings for LLM context
        self.findings_map = {}  # task_id -> VAFinding

        # Action/observation spaces
        self.num_tasks = len(self.masker.tasks)
        self.action_space = spaces.Discrete(self.num_tasks)

        obs_size = (
            self.num_tasks * 5 +  # Task features: priority, cvss, completed, available, attempted
            3                      # Global: progress, accessible_hosts, step
        )
        self.observation_space = spaces.Box(
            low=0.0, high=1.0, shape=(obs_size,), dtype=np.float32
        )

        # Episode tracking
        self.current_step = 0
        self.episode_tasks_completed = 0
        self.episode_successes = 0
        self.task_attempts = {}  # task_id -> num_attempts

    def reset(self, seed=None, options=None):
        """Reset environment for new episode"""
        super().reset(seed=seed)

        # Reset masker state
        self.masker.completed_tasks = set()
        self.masker.current_access = set()
        self.masker.compromised_credentials = {}

        for task in self.masker.tasks:
            task.is_completed = False
            task.is_available = False
            self.masker.current_access.add(task.host)

        self.current_step = 0
        self.episode_tasks_completed = 0
        self.episode_successes = 0
        self.task_attempts = {}

        return self._get_observation(), self._get_info()

    def step(self, action: int):
        """Execute action with LLM script generation"""
        self.current_step += 1

        task = self.masker.get_task_by_index(action)
        action_mask = self.masker.get_action_mask()

        # Check if action is valid
        if task is None or action_mask[action] == 0:
            # Invalid action
            reward = self.reward_failure * 2
            info = {
                'action_valid': False,
                'task_completed': False,
                'reason': 'Invalid action (masked or completed)'
            }
            return self._get_observation(), reward, False, False, info

        # Track attempts
        self.task_attempts[task.task_id] = self.task_attempts.get(task.task_id, 0) + 1

        # Execute exploit (with or without LLM)
        if self.use_llm and self.llm_bridge:
            success, exec_result = self._execute_with_llm(task)
        else:
            success = self._simulate_exploit(task)
            exec_result = None

        # Calculate reward
        if success:
            risk_bonus = (task.cvss_score / 10.0) * 50
            reward = self.reward_success + risk_bonus + self.reward_step
            self.episode_successes += 1
        else:
            reward = self.reward_failure + self.reward_step

        # Mark task completed
        self.masker.mark_completed(task.task_id, success)
        self.episode_tasks_completed += 1

        # Build info
        info = {
            'action_valid': True,
            'task_completed': True,
            'success': success,
            'task_id': task.task_id,
            'cve': task.cve,
            'cvss': task.cvss_score,
            'host': task.host,
            'attempts': self.task_attempts[task.task_id]
        }

        if exec_result:
            info['execution_result'] = {
                'exit_code': exec_result.exit_code,
                'duration': exec_result.duration,
                'output_length': len(exec_result.stdout)
            }

        # Check termination
        terminated = self.episode_tasks_completed >= self.num_tasks
        truncated = self.current_step >= self.max_steps

        if terminated:
            info['episode_complete'] = True
            info['success_rate'] = self.episode_successes / max(self.episode_tasks_completed, 1)

        return self._get_observation(), reward, terminated, truncated, info

    def _execute_with_llm(self, task) -> Tuple[bool, Optional[ExecutionResult]]:
        """
        Execute task using LLM-generated script.

        Flow:
        1. Get corresponding VAFinding for context
        2. Check if similar script exists in memory
        3. If not, use LLM to generate new script
        4. Execute script in sandbox
        5. Store result in memory if successful
        """
        # Get VAFinding for this task (needed for LLM context)
        finding = self.findings_map.get(task.task_id)

        if not finding:
            if self.verbose:
                print(f"⚠ No finding for task {task.task_id}, using simulation")
            return self._simulate_exploit(task), None

        # Try to get script from memory first
        existing_script = self.llm_bridge.get_script_from_memory(finding, task)

        if existing_script and self.verbose:
            print(f"✓ Using cached script for {task.cve}")

        # Execute via LLM-DRL bridge (handles generation + execution)
        success, exec_result, script = self.llm_bridge.plan_and_execute(finding, task)

        return success, exec_result

    def _simulate_exploit(self, task) -> bool:
        """Probability-based simulation (fallback)"""
        cvss = task.cvss_score
        attempts = self.task_attempts.get(task.task_id, 0)

        # Base probability by CVSS
        if cvss >= 9.0:
            prob = 0.90
        elif cvss >= 7.0:
            prob = 0.75
        elif cvss >= 4.0:
            prob = 0.60
        else:
            prob = 0.40

        # Reduce probability with more attempts
        prob *= (0.9 ** attempts)

        return np.random.random() < prob

    def _get_observation(self) -> np.ndarray:
        """Build observation vector"""
        obs = []

        # Per-task features
        for task in self.masker.tasks:
            obs.extend([
                task.priority_score / 100.0,
                task.cvss_score / 10.0,
                1.0 if task.is_completed else 0.0,
                1.0 if task.is_available else 0.0,
                min(self.task_attempts.get(task.task_id, 0) / 3.0, 1.0)
            ])

        # Global features
        progress = len(self.masker.completed_tasks) / max(self.num_tasks, 1)
        access_ratio = len(self.masker.current_access) / max(self.num_tasks, 1)
        step_norm = self.current_step / max(self.max_steps, 1)

        obs.extend([progress, access_ratio, step_norm])

        return np.array(obs, dtype=np.float32)

    def _get_info(self) -> dict:
        """Get environment info"""
        return {
            'step': self.current_step,
            'tasks_completed': self.episode_tasks_completed,
            'tasks_succeeded': self.episode_successes,
            'accessible_hosts': len(self.masker.current_access)
        }

    def action_masks(self) -> np.ndarray:
        """Return action mask for invalid action masking"""
        return self.masker.get_action_mask()

    def set_findings_map(self, findings_map: Dict[str, VAFinding]):
        """Set findings map for LLM context"""
        self.findings_map = findings_map


class ProgressCallback(BaseCallback):
    """Callback for tracking training progress"""

    def __init__(self, verbose=1):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_success_rates = []

    def _on_step(self) -> bool:
        # Log episode metrics
        if self.locals.get('dones', [False])[0]:
            info = self.locals.get('infos', [{}])[0]

            if 'success_rate' in info:
                self.episode_success_rates.append(info['success_rate'])
                self.logger.record('pentest/success_rate', info['success_rate'])

            if 'tasks_succeeded' in info:
                self.logger.record('pentest/tasks_succeeded', info['tasks_succeeded'])

            if self.verbose and len(self.episode_success_rates) % 10 == 0:
                avg_success = np.mean(self.episode_success_rates[-10:])
                print(f"\n[Episode {len(self.episode_success_rates)}] "
                      f"Avg Success Rate (last 10): {avg_success:.2%}")

        return True


def prepare_pentest_tasks(nessus_file: str, output_dir: Path):
    """
    Prepare pentest tasks from Nessus scan.

    This runs the full AUVAP pipeline:
    1. Parse Nessus XML
    2. Apply policy filters
    3. Classify with LLM
    4. Generate exploit tasks
    5. Create priority manifest
    """
    print("\n" + "="*70)
    print("PREPARING PENTEST TASKS")
    print("="*70)

    # Parse Nessus scan
    print(f"\n[1/4] Parsing Nessus scan: {nessus_file}")
    findings = parse_nessus_xml(nessus_file)
    print(f"  ✓ Found {len(findings)} vulnerability findings")

    # Initialize task manager
    print(f"\n[2/4] Creating exploit tasks...")
    task_manager = TaskManager()

    for finding in findings:
        task = task_manager.create_task_from_finding(finding)

    print(f"  ✓ Created {len(task_manager.tasks)} exploit tasks")

    # Generate exploits
    print(f"\n[3/4] Generating exploit templates...")
    exploit_gen = ExploitGenerator()

    for task in task_manager.tasks:
        # Generate placeholder scripts
        script_path = output_dir / "exploits" / f"{task.task_id}.py"
        script_path.parent.mkdir(parents=True, exist_ok=True)

        # This would normally use LLM, but we'll create a placeholder
        task.exploit_script = str(script_path)

    print(f"  ✓ Generated {len(task_manager.tasks)} exploit templates")

    # Save manifests
    print(f"\n[4/4] Saving task manifests...")
    experiment_report = output_dir / "experiment_report.json"
    exploits_manifest = output_dir / "exploits_manifest.json"

    task_manager.save_tasks(str(exploits_manifest))

    print(f"  ✓ Saved to {output_dir}")
    print(f"    - experiment_report.json")
    print(f"    - exploits_manifest.json")

    return findings, task_manager, str(experiment_report), str(exploits_manifest)


def train_ppo_llm_pentest(
    nessus_file: str,
    total_timesteps: int = 100000,
    use_llm_generation: bool = True,
    use_real_execution: bool = False,
    llm_provider: str = "openai",
    learning_rate: float = 3e-4,
    n_steps: int = 2048,
    batch_size: int = 64,
    save_dir: str = "./checkpoints",
    log_dir: str = "./logs",
    seed: int = 42,
    verbose: bool = True
):
    """
    Main training function.

    Args:
        nessus_file: Path to Nessus XML scan
        total_timesteps: Training timesteps
        use_llm_generation: Use LLM to generate scripts
        use_real_execution: Execute real scripts (vs simulation)
        llm_provider: LLM provider (openai, gemini, local)
        learning_rate: PPO learning rate
        n_steps: Steps per update
        batch_size: Minibatch size
        save_dir: Checkpoint directory
        log_dir: TensorBoard logs
        seed: Random seed
        verbose: Print detailed logs
    """
    run_name = f"ppo_llm_pentest_{datetime.now().strftime('%Y%m%d_%H%M%S')}"

    print("\n" + "="*70)
    print("PPO PENTEST TRAINING WITH LLM SCRIPT GENERATION")
    print("="*70)
    print(f"Run: {run_name}")
    print(f"Nessus scan: {nessus_file}")
    print(f"LLM generation: {'ENABLED' if use_llm_generation else 'DISABLED'}")
    print(f"Real execution: {'ENABLED' if use_real_execution else 'DISABLED (simulation)'}")
    print(f"LLM provider: {llm_provider}")
    print(f"Timesteps: {total_timesteps}")
    print("="*70)

    # Create directories
    save_dir = Path(save_dir)
    log_dir = Path(log_dir)
    output_dir = Path("./pentest_tasks")

    for d in [save_dir, log_dir, output_dir]:
        d.mkdir(parents=True, exist_ok=True)

    # Prepare tasks
    findings, task_manager, experiment_report, exploits_manifest = prepare_pentest_tasks(
        nessus_file, output_dir
    )

    # Initialize components
    print("\n" + "="*70)
    print("INITIALIZING COMPONENTS")
    print("="*70)

    # Priority masker (sequential mode)
    print(f"\n[1/4] Initializing priority masker (SEQUENTIAL mode)...")
    masker = PriorityMasker(experiment_report, exploits_manifest, sequential_mode=True)
    masker.print_status()

    # LLM-DRL Bridge (if using LLM)
    llm_bridge = None
    if use_llm_generation:
        print(f"\n[2/4] Initializing LLM-DRL bridge...")
        sandbox = SandboxExecutor(
            use_docker=use_real_execution,
            network_isolation=True,
            timeout=30
        )
        memory = PersistentMemory(db_path="./pentest_memory.db")

        llm_bridge = LLMDRLBridge(
            sandbox_executor=sandbox,
            persistent_memory=memory,
            llm_provider=llm_provider,
            max_refinement_iterations=3,
            verbose=verbose
        )
        print(f"  ✓ LLM bridge ready")
    else:
        print(f"\n[2/4] LLM generation disabled, using simulation")

    # Create environment
    print(f"\n[3/4] Creating training environment...")

    def make_env():
        env = LLMPentestEnv(
            priority_masker=masker,
            llm_bridge=llm_bridge,
            use_llm_generation=use_llm_generation,
            use_real_execution=use_real_execution,
            max_steps=len(masker.tasks) * 3,
            verbose=verbose
        )

        # Set findings map for LLM context
        findings_map = {task.task_id: finding
                       for task, finding in zip(task_manager.tasks, findings)}
        env.set_findings_map(findings_map)

        return env

    env = DummyVecEnv([make_env])
    print(f"  ✓ Environment created")

    # Create PPO model
    print(f"\n[4/4] Initializing PPO model...")
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=0.99,
        verbose=1,
        tensorboard_log=str(log_dir / run_name),
        seed=seed
    )
    print(f"  ✓ PPO model ready")
    print(f"  - Policy: MlpPolicy")
    print(f"  - Parameters: {sum(p.numel() for p in model.policy.parameters()):,}")

    # Train
    print("\n" + "="*70)
    print("STARTING TRAINING")
    print("="*70)
    print(f"Monitor with: tensorboard --logdir {log_dir}")
    print("="*70 + "\n")

    callback = ProgressCallback(verbose=1)

    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=callback,
            log_interval=10,
            tb_log_name="ppo_llm_pentest",
            progress_bar=True
        )

        # Save model
        final_path = save_dir / run_name / "final_model"
        final_path.parent.mkdir(parents=True, exist_ok=True)
        model.save(final_path)

        print(f"\n✓ Training complete! Model saved to: {final_path}")

    except KeyboardInterrupt:
        print("\n\nTraining interrupted by user")
        interrupted_path = save_dir / run_name / "interrupted_model"
        interrupted_path.parent.mkdir(parents=True, exist_ok=True)
        model.save(interrupted_path)
        print(f"✓ Model saved to: {interrupted_path}")

    # Final stats
    print("\n" + "="*70)
    print("TRAINING COMPLETE")
    print("="*70)

    if callback.episode_success_rates:
        print(f"Episodes completed: {len(callback.episode_success_rates)}")
        print(f"Avg success rate (last 50): {np.mean(callback.episode_success_rates[-50:]):.2%}")

    masker.print_status()

    env.close()

    return model


def main():
    """CLI entry point"""
    parser = argparse.ArgumentParser(
        description="Train PPO for pentest with LLM script generation"
    )

    # Required
    parser.add_argument("--nessus-scan", type=str, required=True,
                       help="Path to Nessus XML scan file")

    # LLM options
    parser.add_argument("--use-llm-generation", action="store_true",
                       help="Use LLM to generate exploit scripts")
    parser.add_argument("--llm-provider", type=str, default="openai",
                       choices=["openai", "gemini", "local"],
                       help="LLM provider")

    # Execution options
    parser.add_argument("--real-execution", action="store_true",
                       help="Execute actual scripts (default: simulation)")

    # Training options
    parser.add_argument("--timesteps", type=int, default=100000,
                       help="Total training timesteps")
    parser.add_argument("--lr", type=float, default=3e-4,
                       help="Learning rate")
    parser.add_argument("--n-steps", type=int, default=2048,
                       help="Steps per update")
    parser.add_argument("--batch-size", type=int, default=64,
                       help="Minibatch size")

    # Paths
    parser.add_argument("--save-dir", type=str, default="./checkpoints",
                       help="Checkpoint directory")
    parser.add_argument("--log-dir", type=str, default="./logs",
                       help="TensorBoard logs")

    # Other
    parser.add_argument("--seed", type=int, default=42,
                       help="Random seed")
    parser.add_argument("--quiet", action="store_true",
                       help="Reduce output verbosity")

    args = parser.parse_args()

    # Train
    model = train_ppo_llm_pentest(
        nessus_file=args.nessus_scan,
        total_timesteps=args.timesteps,
        use_llm_generation=args.use_llm_generation,
        use_real_execution=args.real_execution,
        llm_provider=args.llm_provider,
        learning_rate=args.lr,
        n_steps=args.n_steps,
        batch_size=args.batch_size,
        save_dir=args.save_dir,
        log_dir=args.log_dir,
        seed=args.seed,
        verbose=not args.quiet
    )


if __name__ == "__main__":
    main()
